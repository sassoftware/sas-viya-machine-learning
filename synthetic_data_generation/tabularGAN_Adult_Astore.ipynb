{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swat as sw\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>?</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt  education  education-num  \\\n",
       "0       39         State-gov   77516  Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2       38           Private  215646    HS-grad              9   \n",
       "3       53           Private  234721       11th              7   \n",
       "4       28           Private  338409  Bachelors             13   \n",
       "...    ...               ...     ...        ...            ...   \n",
       "48837   39           Private  215419  Bachelors             13   \n",
       "48838   64                 ?  321403    HS-grad              9   \n",
       "48839   38           Private  374983  Bachelors             13   \n",
       "48840   44           Private   83891  Bachelors             13   \n",
       "48841   35      Self-emp-inc  182148  Bachelors             13   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           Never-married       Adm-clerical   Not-in-family   \n",
       "1      Married-civ-spouse    Exec-managerial         Husband   \n",
       "2                Divorced  Handlers-cleaners   Not-in-family   \n",
       "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
       "4      Married-civ-spouse     Prof-specialty            Wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            Divorced     Prof-specialty   Not-in-family   \n",
       "48838             Widowed                  ?  Other-relative   \n",
       "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
       "48840            Divorced       Adm-clerical       Own-child   \n",
       "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   White    Male          2174             0              40   \n",
       "1                   White    Male             0             0              13   \n",
       "2                   White    Male             0             0              40   \n",
       "3                   Black    Male             0             0              40   \n",
       "4                   Black  Female             0             0              40   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               White  Female             0             0              36   \n",
       "48838               Black    Male             0             0              40   \n",
       "48839               White    Male             0             0              50   \n",
       "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
       "48841               White    Male             0             0              60   \n",
       "\n",
       "      native-country  label  \n",
       "0      United-States  <=50K  \n",
       "1      United-States  <=50K  \n",
       "2      United-States  <=50K  \n",
       "3      United-States  <=50K  \n",
       "4               Cuba  <=50K  \n",
       "...              ...    ...  \n",
       "48837  United-States  <=50K  \n",
       "48838  United-States  <=50K  \n",
       "48839  United-States  <=50K  \n",
       "48840  United-States  <=50K  \n",
       "48841  United-States   >50K  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_data = pd.read_csv('../gan-testing/data/adult.csv')\n",
    "adult_discrete_columns = \"workclass,education,marital-status,occupation,relationship,race,sex,native-country,label\".split(',')\n",
    "adult_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  label  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove some extra charachters\n",
    "adult_data = adult_data.replace({'\\$': '', ',': ''}, regex=True)\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the target >50K & <=50K with 1, 0\n",
    "adult_data.loc[adult_data['label']=='>50K', 'label'] = 1\n",
    "adult_data.loc[adult_data['label']=='<=50K', 'label'] = 0 # use this as input to the GAN\n",
    "unique_labels = adult_data['label'].unique()\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data.replace('?', np.nan, inplace=True)\n",
    "adult_data=adult_data.fillna(adult_data.mean())\n",
    "adult_data = adult_data.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>education_10th</th>\n",
       "      <th>education_11th</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass_Federal-gov  workclass_Local-gov  workclass_Never-worked  \\\n",
       "0                      0                    0                       0   \n",
       "1                      0                    0                       0   \n",
       "2                      0                    0                       0   \n",
       "3                      0                    0                       0   \n",
       "4                      0                    0                       0   \n",
       "\n",
       "   workclass_Private  workclass_Self-emp-inc  workclass_Self-emp-not-inc  \\\n",
       "0                  0                       0                           0   \n",
       "1                  0                       0                           1   \n",
       "2                  1                       0                           0   \n",
       "3                  1                       0                           0   \n",
       "4                  1                       0                           0   \n",
       "\n",
       "   workclass_State-gov  workclass_Without-pay  education_10th  education_11th  \\\n",
       "0                    1                      0               0               0   \n",
       "1                    0                      0               0               0   \n",
       "2                    0                      0               0               0   \n",
       "3                    0                      0               0               1   \n",
       "4                    0                      0               0               0   \n",
       "\n",
       "   ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0  ...                        0                           0   \n",
       "1  ...                        0                           0   \n",
       "2  ...                        0                           0   \n",
       "3  ...                        0                           0   \n",
       "4  ...                        0                           0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        0                     0                      0   \n",
       "1                        0                     0                      0   \n",
       "2                        0                     0                      0   \n",
       "3                        0                     0                      0   \n",
       "4                        0                     0                      0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        0                               0   \n",
       "1                        0                               0   \n",
       "2                        0                               0   \n",
       "3                        0                               0   \n",
       "4                        0                               0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                             1                       0   \n",
       "1                             1                       0   \n",
       "2                             1                       0   \n",
       "3                             1                       0   \n",
       "4                             0                       0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the Ruiwen's Encoder for ML utility and CTGAN transformer for GAN\n",
    "str_cols= [ 'workclass', 'education', 'marital-status', 'relationship','race', 'sex','native-country']\n",
    "num_cols = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week', 'label']\n",
    "dataframe = pd.DataFrame(adult_data.loc[:,str_cols])\n",
    "\n",
    "one_hot_columns = pd.DataFrame()\n",
    "for col_name, item in dataframe.iteritems():\n",
    "    \n",
    "    #print(col_name)\n",
    "    #print(item)\n",
    "    col = pd.get_dummies(item, prefix=col_name)\n",
    "    one_hot_columns =pd.concat([one_hot_columns,col],axis=1)\n",
    "one_hot_columns.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire data is concat of discrete and contiuous cols\n",
    "adult_data_all = pd.concat([one_hot_columns,adult_data.loc[:,num_cols]],axis=1)\n",
    "#adult_data_all.head()\n",
    "my_X = adult_data_all.drop([\"label\"],axis=1)\n",
    "orig_X, orig_y = my_X,adult_data_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "orig_X_train, orig_X_test, orig_y_train, orig_y_test = train_test_split(orig_X, orig_y, test_size=0.3, random_state=123)\n",
    "my_data1_train = pd.concat([orig_X_train,orig_y_train],axis=1)\n",
    "my_data1_test = pd.concat([orig_X_test,orig_y_test],axis=1)\n",
    "#my_data1_test.describe()\n",
    "orig_y_train = orig_y_train.astype('int')\n",
    "orig_y_test = orig_y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ML models on the train set of the original data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "names = [\"Decision Tree\",\"Linear SVM\", \"Random Forest\", \"Logistic Regression\",\"MLP\"]\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the original data:\n",
      "Decision Tree Acc:  0.8537500853067631 f-1:  0.6328593455542231 AUC: 0.8863975382887572\n",
      "Linear SVM Acc:  0.2956391182692964 f-1:  0.368089144676422 AUC: 0.509806196049388\n",
      "Random Forest Acc:  0.8495188698560022 f-1:  0.6575555210436402 AUC: 0.8977741254446227\n",
      "Logistic Regression Acc:  0.80017743806729 f-1:  0.3851322973540529 AUC: 0.5769572159245973\n",
      "MLP Acc:  0.7840715211902 f-1:  0.43419170243204575 AUC: 0.668529384827481\n"
     ]
    }
   ],
   "source": [
    "print('ML scores for the original data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(orig_X_train, orig_y_train)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workclass',\n",
       " 'education',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'native-country',\n",
       " 'label']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # now input the data to CPCTGAN and train\n",
    "# adult_discrete_columns = adult_discrete_columns[:-1]\n",
    "adult_discrete_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train data for gan with same seed as the ML utility\n",
    "GAN_X = adult_data.drop([\"label\"],axis=1)\n",
    "GAN_orig_X, GAN_orig_y = GAN_X,adult_data.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "GAN_X_train, GAN_X_test, GAN_y_train, GAN_y_test = train_test_split(GAN_orig_X, GAN_orig_y, test_size=0.3, random_state=123)\n",
    "GAN_data_train = pd.concat([GAN_X_train,GAN_y_train],axis=1)\n",
    "GAN_data_test = pd.concat([GAN_X_test,GAN_y_test],axis=1)\n",
    "#my_data1_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sw.CAS('dl2073.clstr.rnd.sas.com',33789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'generativeAdversarialNet'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>generativeAdversarialNet</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.339s</span> &#183; <span class=\"cas-user\">user 3.55s</span> &#183; <span class=\"cas-sys\">sys 2.91s</span> &#183; <span class=\"cas-memory\">mem 0.222MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'generativeAdversarialNet'\n",
       "\n",
       "+ Elapsed: 0.339s, user: 3.55s, sys: 2.91s, mem: 0.222mb"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loadactionset('generativeAdversarialNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table GAN_DATA_TRAIN in caslib CASUSER(alphel).\n",
      "NOTE: The table GAN_DATA_TRAIN has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASUSER(alphel)</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>GAN_DATA_TRAIN</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('GAN_DATA_TRAIN', caslib='CASUSER(alphel)')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.103s</span> &#183; <span class=\"cas-user\">user 0.273s</span> &#183; <span class=\"cas-sys\">sys 0.0503s</span> &#183; <span class=\"cas-memory\">mem 126MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'CASUSER(alphel)'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'GAN_DATA_TRAIN'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('GAN_DATA_TRAIN', caslib='CASUSER(alphel)')\n",
       "\n",
       "+ Elapsed: 0.103s, user: 0.273s, sys: 0.0503s, mem: 126mb"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upload(GAN_data_train, casout=dict(name='GAN_data_train', replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table CEN in caslib CASUSER(alphel).\n",
      "NOTE: The table CEN has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VarName</th>\n",
       "      <th>Centroid-i</th>\n",
       "      <th>weight</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19322</td>\n",
       "      <td>27.77</td>\n",
       "      <td>11.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>2</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05051</td>\n",
       "      <td>53.80</td>\n",
       "      <td>6.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>4</td>\n",
       "      <td>0.13165</td>\n",
       "      <td>52.02</td>\n",
       "      <td>20.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05105</td>\n",
       "      <td>53.95</td>\n",
       "      <td>7.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10788</td>\n",
       "      <td>48.49</td>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>age</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14270</td>\n",
       "      <td>31.37</td>\n",
       "      <td>8.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18711</td>\n",
       "      <td>26.35</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>age</td>\n",
       "      <td>3</td>\n",
       "      <td>0.16313</td>\n",
       "      <td>43.14</td>\n",
       "      <td>12.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>age</td>\n",
       "      <td>4</td>\n",
       "      <td>0.14163</td>\n",
       "      <td>36.40</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>age</td>\n",
       "      <td>5</td>\n",
       "      <td>0.21957</td>\n",
       "      <td>49.69</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>age</td>\n",
       "      <td>6</td>\n",
       "      <td>0.14585</td>\n",
       "      <td>29.30</td>\n",
       "      <td>7.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19857</td>\n",
       "      <td>134121.87</td>\n",
       "      <td>61429.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07079</td>\n",
       "      <td>361429.52</td>\n",
       "      <td>185644.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>3</td>\n",
       "      <td>0.21485</td>\n",
       "      <td>287922.30</td>\n",
       "      <td>90902.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>4</td>\n",
       "      <td>0.15934</td>\n",
       "      <td>160820.85</td>\n",
       "      <td>66425.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>5</td>\n",
       "      <td>0.20436</td>\n",
       "      <td>127945.67</td>\n",
       "      <td>60218.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>6</td>\n",
       "      <td>0.15208</td>\n",
       "      <td>175072.62</td>\n",
       "      <td>71618.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>education-num</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21242</td>\n",
       "      <td>7.33</td>\n",
       "      <td>3.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>education-num</td>\n",
       "      <td>2</td>\n",
       "      <td>0.47952</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>education-num</td>\n",
       "      <td>3</td>\n",
       "      <td>0.30806</td>\n",
       "      <td>11.98</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91891</td>\n",
       "      <td>0.05</td>\n",
       "      <td>42.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>2</td>\n",
       "      <td>0.02360</td>\n",
       "      <td>6896.89</td>\n",
       "      <td>1386.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00491</td>\n",
       "      <td>99413.59</td>\n",
       "      <td>7609.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>4</td>\n",
       "      <td>0.02150</td>\n",
       "      <td>14477.55</td>\n",
       "      <td>5509.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>5</td>\n",
       "      <td>0.03108</td>\n",
       "      <td>3114.35</td>\n",
       "      <td>1197.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03581</td>\n",
       "      <td>1834.96</td>\n",
       "      <td>208.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.95209</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01120</td>\n",
       "      <td>2103.41</td>\n",
       "      <td>542.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>542.76</td>\n",
       "      <td>229.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>label</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23984</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>label</td>\n",
       "      <td>2</td>\n",
       "      <td>0.76016</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           VarName  Centroid-i   weight       Mean        Std\n",
       "0   hours-per-week           1  0.19322      27.77      11.92\n",
       "1   hours-per-week           2  0.46569      40.00       0.10\n",
       "2   hours-per-week           3  0.05051      53.80       6.97\n",
       "3   hours-per-week           4  0.13165      52.02      20.72\n",
       "4   hours-per-week           5  0.05105      53.95       7.23\n",
       "5   hours-per-week           6  0.10788      48.49       2.39\n",
       "6              age           1  0.14270      31.37       8.39\n",
       "7              age           2  0.18711      26.35       5.90\n",
       "8              age           3  0.16313      43.14      12.26\n",
       "9              age           4  0.14163      36.40      10.26\n",
       "10             age           5  0.21957      49.69      13.50\n",
       "11             age           6  0.14585      29.30       7.47\n",
       "12          fnlwgt           1  0.19857  134121.87   61429.73\n",
       "13          fnlwgt           2  0.07079  361429.52  185644.38\n",
       "14          fnlwgt           3  0.21485  287922.30   90902.78\n",
       "15          fnlwgt           4  0.15934  160820.85   66425.34\n",
       "16          fnlwgt           5  0.20436  127945.67   60218.04\n",
       "17          fnlwgt           6  0.15208  175072.62   71618.94\n",
       "18   education-num           1  0.21242       7.33       3.24\n",
       "19   education-num           2  0.47952       9.30       0.47\n",
       "20   education-num           3  0.30806      11.98       2.08\n",
       "21    capital-gain           1  0.91891       0.05      42.18\n",
       "22    capital-gain           2  0.02360    6896.89    1386.30\n",
       "23    capital-gain           3  0.00491   99413.59    7609.05\n",
       "24    capital-gain           4  0.02150   14477.55    5509.19\n",
       "25    capital-gain           5  0.03108    3114.35    1197.44\n",
       "26    capital-loss           1  0.03581    1834.96     208.73\n",
       "27    capital-loss           2  0.95209       0.00       2.32\n",
       "28    capital-loss           3  0.01120    2103.41     542.26\n",
       "29    capital-loss           4  0.00090     542.76     229.63\n",
       "30           label           1  0.23984       1.00       0.01\n",
       "31           label           2  0.76016       0.00       0.00"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cen = pd.read_csv(\"../gan-testing/data/adult_centroids.csv\")\n",
    "s.upload(cen, casout=dict(name='cen', replace=True))\n",
    "cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using device: GPU 0.\n",
      "NOTE: Epoch i=1, ae_loss=  0.0512.\n",
      "NOTE: Epoch i=2, ae_loss=  0.0392.\n",
      "NOTE: Epoch i=3, ae_loss=  0.0336.\n",
      "NOTE: Epoch i=4, ae_loss=  0.0289.\n",
      "NOTE: Epoch i=5, ae_loss=  0.0266.\n",
      "NOTE: Epoch i=6, ae_loss=  0.0221.\n",
      "NOTE: Epoch i=7, ae_loss=  0.0208.\n",
      "NOTE: Epoch i=8, ae_loss=  0.0191.\n",
      "NOTE: Epoch i=9, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=10, ae_loss=  0.0147.\n",
      "NOTE: Epoch i=11, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=12, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=13, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=14, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=15, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=16, ae_loss=  0.0106.\n",
      "NOTE: Epoch i=17, ae_loss=  0.0110.\n",
      "NOTE: Epoch i=18, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=19, ae_loss=  0.0110.\n",
      "NOTE: Epoch i=20, ae_loss=  0.0113.\n",
      "NOTE: Epoch i=21, ae_loss=  0.0117.\n",
      "NOTE: Epoch i=22, ae_loss=  0.0102.\n",
      "NOTE: Epoch i=23, ae_loss=  0.0106.\n",
      "NOTE: Epoch i=24, ae_loss=  0.0096.\n",
      "NOTE: Epoch i=25, ae_loss=  0.0104.\n",
      "NOTE: Epoch i=26, ae_loss=  0.0105.\n",
      "NOTE: Epoch i=27, ae_loss=  0.0101.\n",
      "NOTE: Epoch i=28, ae_loss=  0.0094.\n",
      "NOTE: Epoch i=29, ae_loss=  0.0107.\n",
      "NOTE: Epoch i=30, ae_loss=  0.0106.\n",
      "NOTE: Epoch i=31, ae_loss=  0.0103.\n",
      "NOTE: Epoch i=32, ae_loss=  0.0109.\n",
      "NOTE: Epoch i=33, ae_loss=  0.0101.\n",
      "NOTE: Epoch i=34, ae_loss=  0.0102.\n",
      "NOTE: Epoch i=35, ae_loss=  0.0099.\n",
      "NOTE: Epoch i=36, ae_loss=  0.0091.\n",
      "NOTE: Epoch i=37, ae_loss=  0.0094.\n",
      "NOTE: Epoch i=38, ae_loss=  0.0101.\n",
      "NOTE: Epoch i=39, ae_loss=  0.0102.\n",
      "NOTE: Epoch i=40, ae_loss=  0.0098.\n",
      "NOTE: Epoch i=41, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=42, ae_loss=  0.0094.\n",
      "NOTE: Epoch i=43, ae_loss=  0.0091.\n",
      "NOTE: Epoch i=44, ae_loss=  0.0091.\n",
      "NOTE: Epoch i=45, ae_loss=  0.0097.\n",
      "NOTE: Epoch i=46, ae_loss=  0.0095.\n",
      "NOTE: Epoch i=47, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=48, ae_loss=  0.0090.\n",
      "NOTE: Epoch i=49, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=50, ae_loss=  0.0099.\n",
      "NOTE: Epoch i=51, ae_loss=  0.0093.\n",
      "NOTE: Epoch i=52, ae_loss=  0.0095.\n",
      "NOTE: Epoch i=53, ae_loss=  0.0093.\n",
      "NOTE: Epoch i=54, ae_loss=  0.0092.\n",
      "NOTE: Epoch i=55, ae_loss=  0.0091.\n",
      "NOTE: Epoch i=56, ae_loss=  0.0091.\n",
      "NOTE: Epoch i=57, ae_loss=  0.0094.\n",
      "NOTE: Epoch i=58, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=59, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=60, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=61, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=62, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=63, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=64, ae_loss=  0.0093.\n",
      "NOTE: Epoch i=65, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=66, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=67, ae_loss=  0.0093.\n",
      "NOTE: Epoch i=68, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=69, ae_loss=  0.0092.\n",
      "NOTE: Epoch i=70, ae_loss=  0.0095.\n",
      "NOTE: Epoch i=71, ae_loss=  0.0080.\n",
      "NOTE: Epoch i=72, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=73, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=74, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=75, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=76, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=77, ae_loss=  0.0094.\n",
      "NOTE: Epoch i=78, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=79, ae_loss=  0.0091.\n",
      "NOTE: Epoch i=80, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=81, ae_loss=  0.0092.\n",
      "NOTE: Epoch i=82, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=83, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=84, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=85, ae_loss=  0.0096.\n",
      "NOTE: Epoch i=86, ae_loss=  0.0090.\n",
      "NOTE: Epoch i=87, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=88, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=89, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=90, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=91, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=92, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=93, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=94, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=95, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=96, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=97, ae_loss=  0.0089.\n",
      "NOTE: Epoch i=98, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=99, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=100, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=101, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=102, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=103, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=104, ae_loss=  0.0081.\n",
      "NOTE: Epoch i=105, ae_loss=  0.0080.\n",
      "NOTE: Epoch i=106, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=107, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=108, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=109, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=110, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=111, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=112, ae_loss=  0.0099.\n",
      "NOTE: Epoch i=113, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=114, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=115, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=116, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=117, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=118, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=119, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=120, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=121, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=122, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=123, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=124, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=125, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=126, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=127, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=128, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=129, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=130, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=131, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=132, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=133, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=134, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=135, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=136, ae_loss=  0.0080.\n",
      "NOTE: Epoch i=137, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=138, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=139, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=140, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=141, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=142, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=143, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=144, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=145, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=146, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=147, ae_loss=  0.0089.\n",
      "NOTE: Epoch i=148, ae_loss=  0.0080.\n",
      "NOTE: Epoch i=149, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=150, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=151, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=152, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=153, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=154, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=155, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=156, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=157, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=158, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=159, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=160, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=161, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=162, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=163, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=164, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=165, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=166, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=167, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=168, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=169, ae_loss=  0.0080.\n",
      "NOTE: Epoch i=170, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=171, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=172, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=173, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=174, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=175, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=176, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=177, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=178, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=179, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=180, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=181, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=182, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=183, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=184, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=185, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=186, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=187, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=188, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=189, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=190, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=191, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=192, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=193, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=194, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=195, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=196, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=197, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=198, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=199, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=200, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=201, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=202, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=203, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=204, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=205, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=206, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=207, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=208, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=209, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=210, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=211, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=212, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=213, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=214, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=215, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=216, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=217, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=218, ae_loss=  0.0080.\n",
      "NOTE: Epoch i=219, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=220, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=221, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=222, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=223, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=224, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=225, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=226, ae_loss=  0.0076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=227, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=228, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=229, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=230, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=231, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=232, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=233, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=234, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=235, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=236, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=237, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=238, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=239, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=240, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=241, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=242, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=243, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=244, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=245, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=246, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=247, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=248, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=249, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=250, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=251, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=252, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=253, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=254, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=255, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=256, ae_loss=  0.0080.\n",
      "NOTE: Epoch i=257, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=258, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=259, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=260, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=261, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=262, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=263, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=264, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=265, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=266, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=267, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=268, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=269, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=270, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=271, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=272, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=273, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=274, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=275, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=276, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=277, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=278, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=279, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=280, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=281, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=282, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=283, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=284, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=285, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=286, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=287, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=288, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=289, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=290, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=291, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=292, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=293, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=294, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=295, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=296, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=297, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=298, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=299, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=300, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=301, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=302, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=303, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=304, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=305, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=306, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=307, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=308, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=309, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=310, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=311, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=312, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=313, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=314, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=315, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=316, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=317, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=318, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=319, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=320, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=321, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=322, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=323, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=324, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=325, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=326, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=327, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=328, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=329, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=330, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=331, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=332, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=333, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=334, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=335, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=336, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=337, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=338, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=339, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=340, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=341, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=342, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=343, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=344, ae_loss=  0.0080.\n",
      "NOTE: Epoch i=345, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=346, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=347, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=348, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=349, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=350, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=351, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=352, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=353, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=354, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=355, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=356, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=357, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=358, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=359, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=360, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=361, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=362, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=363, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=364, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=365, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=366, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=367, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=368, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=369, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=370, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=371, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=372, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=373, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=374, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=375, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=376, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=377, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=378, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=379, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=380, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=381, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=382, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=383, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=384, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=385, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=386, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=387, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=388, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=389, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=390, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=391, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=392, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=393, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=394, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=395, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=396, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=397, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=398, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=399, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=400, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=401, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=402, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=403, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=404, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=405, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=406, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=407, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=408, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=409, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=410, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=411, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=412, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=413, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=414, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=415, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=416, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=417, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=418, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=419, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=420, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=421, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=422, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=423, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=424, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=425, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=426, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=427, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=428, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=429, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=430, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=431, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=432, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=433, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=434, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=435, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=436, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=437, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=438, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=439, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=440, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=441, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=442, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=443, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=444, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=445, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=446, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=447, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=448, ae_loss=  0.0067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=449, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=450, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=451, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=452, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=453, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=454, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=455, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=456, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=457, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=458, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=459, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=460, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=461, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=462, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=463, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=464, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=465, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=466, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=467, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=468, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=469, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=470, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=471, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=472, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=473, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=474, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=475, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=476, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=477, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=478, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=479, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=480, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=481, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=482, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=483, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=484, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=485, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=486, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=487, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=488, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=489, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=490, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=491, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=492, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=493, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=494, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=495, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=496, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=497, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=498, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=499, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=500, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=1, g_loss=  2.0902, d_loss= -0.0148.\n",
      "NOTE: Epoch i=2, g_loss=  2.0732, d_loss= -0.1764.\n",
      "NOTE: Epoch i=3, g_loss=  2.0993, d_loss= -0.3064.\n",
      "NOTE: Epoch i=4, g_loss=  2.2871, d_loss= -0.4730.\n",
      "NOTE: Epoch i=5, g_loss=  2.2221, d_loss= -0.6208.\n",
      "NOTE: Epoch i=6, g_loss=  2.2991, d_loss= -0.8559.\n",
      "NOTE: Epoch i=7, g_loss=  2.3148, d_loss= -1.0565.\n",
      "NOTE: Epoch i=8, g_loss=  2.4204, d_loss= -1.0855.\n",
      "NOTE: Epoch i=9, g_loss=  2.4033, d_loss= -1.1557.\n",
      "NOTE: Epoch i=10, g_loss=  2.6052, d_loss= -1.1896.\n",
      "NOTE: Epoch i=11, g_loss=  2.5201, d_loss= -1.1234.\n",
      "NOTE: Epoch i=12, g_loss=  2.4384, d_loss= -1.2802.\n",
      "NOTE: Epoch i=13, g_loss=  2.7351, d_loss= -1.2791.\n",
      "NOTE: Epoch i=14, g_loss=  2.5364, d_loss= -1.1577.\n",
      "NOTE: Epoch i=15, g_loss=  2.7770, d_loss= -1.2119.\n",
      "NOTE: Epoch i=16, g_loss=  2.7769, d_loss= -1.2615.\n",
      "NOTE: Epoch i=17, g_loss=  2.8992, d_loss= -1.2485.\n",
      "NOTE: Epoch i=18, g_loss=  2.9505, d_loss= -1.0176.\n",
      "NOTE: Epoch i=19, g_loss=  2.9056, d_loss= -1.2597.\n",
      "NOTE: Epoch i=20, g_loss=  3.0622, d_loss= -1.0663.\n",
      "NOTE: Epoch i=21, g_loss=  3.0347, d_loss= -1.0645.\n",
      "NOTE: Epoch i=22, g_loss=  3.0483, d_loss= -1.1445.\n",
      "NOTE: Epoch i=23, g_loss=  3.1769, d_loss= -0.9842.\n",
      "NOTE: Epoch i=24, g_loss=  3.2576, d_loss= -1.2549.\n",
      "NOTE: Epoch i=25, g_loss=  3.2732, d_loss= -0.8878.\n",
      "NOTE: Epoch i=26, g_loss=  3.4056, d_loss= -0.9183.\n",
      "NOTE: Epoch i=27, g_loss=  3.5863, d_loss= -0.7865.\n",
      "NOTE: Epoch i=28, g_loss=  3.6733, d_loss= -1.1811.\n",
      "NOTE: Epoch i=29, g_loss=  3.6287, d_loss= -0.7976.\n",
      "NOTE: Epoch i=30, g_loss=  3.7894, d_loss= -0.9622.\n",
      "NOTE: Epoch i=31, g_loss=  3.7238, d_loss= -0.6721.\n",
      "NOTE: Epoch i=32, g_loss=  3.8691, d_loss= -1.1830.\n",
      "NOTE: Epoch i=33, g_loss=  3.8176, d_loss= -1.1109.\n",
      "NOTE: Epoch i=34, g_loss=  3.9952, d_loss= -0.8819.\n",
      "NOTE: Epoch i=35, g_loss=  3.9598, d_loss= -0.7684.\n",
      "NOTE: Epoch i=36, g_loss=  3.8173, d_loss= -0.9081.\n",
      "NOTE: Epoch i=37, g_loss=  3.9920, d_loss= -0.9737.\n",
      "NOTE: Epoch i=38, g_loss=  4.0090, d_loss= -1.0304.\n",
      "NOTE: Epoch i=39, g_loss=  4.0283, d_loss= -0.9258.\n",
      "NOTE: Epoch i=40, g_loss=  3.9859, d_loss= -0.6931.\n",
      "NOTE: Epoch i=41, g_loss=  3.5998, d_loss= -0.7033.\n",
      "NOTE: Epoch i=42, g_loss=  3.4406, d_loss= -0.5933.\n",
      "NOTE: Epoch i=43, g_loss=  3.4568, d_loss= -0.5550.\n",
      "NOTE: Epoch i=44, g_loss=  3.1139, d_loss= -0.9088.\n",
      "NOTE: Epoch i=45, g_loss=  2.8854, d_loss= -0.7127.\n",
      "NOTE: Epoch i=46, g_loss=  2.5415, d_loss= -0.7537.\n",
      "NOTE: Epoch i=47, g_loss=  2.5493, d_loss= -0.7617.\n",
      "NOTE: Epoch i=48, g_loss=  2.1793, d_loss= -0.4390.\n",
      "NOTE: Epoch i=49, g_loss=  2.0005, d_loss= -0.3040.\n",
      "NOTE: Epoch i=50, g_loss=  1.6630, d_loss= -0.2777.\n",
      "NOTE: Epoch i=51, g_loss=  1.5549, d_loss= -0.2192.\n",
      "NOTE: Epoch i=52, g_loss=  1.2258, d_loss= -0.1859.\n",
      "NOTE: Epoch i=53, g_loss=  1.3165, d_loss= -0.1471.\n",
      "NOTE: Epoch i=54, g_loss=  1.3612, d_loss= -0.0284.\n",
      "NOTE: Epoch i=55, g_loss=  0.9480, d_loss= -0.0448.\n",
      "NOTE: Epoch i=56, g_loss=  0.7975, d_loss= -0.0610.\n",
      "NOTE: Epoch i=57, g_loss=  1.0381, d_loss= -0.1700.\n",
      "NOTE: Epoch i=58, g_loss=  1.1457, d_loss= -0.0371.\n",
      "NOTE: Epoch i=59, g_loss=  1.1266, d_loss= -0.2660.\n",
      "NOTE: Epoch i=60, g_loss=  1.1452, d_loss= -0.2731.\n",
      "NOTE: Epoch i=61, g_loss=  1.0877, d_loss= -0.4198.\n",
      "NOTE: Epoch i=62, g_loss=  1.1377, d_loss= -0.5261.\n",
      "NOTE: Epoch i=63, g_loss=  1.2049, d_loss= -0.5226.\n",
      "NOTE: Epoch i=64, g_loss=  1.2451, d_loss= -0.2722.\n",
      "NOTE: Epoch i=65, g_loss=  1.1060, d_loss= -0.1533.\n",
      "NOTE: Epoch i=66, g_loss=  1.1393, d_loss= -0.0728.\n",
      "NOTE: Epoch i=67, g_loss=  0.8936, d_loss= -0.1377.\n",
      "NOTE: Epoch i=68, g_loss=  1.0450, d_loss= -0.0428.\n",
      "NOTE: Epoch i=69, g_loss=  1.0696, d_loss=  0.0770.\n",
      "NOTE: Epoch i=70, g_loss=  1.0499, d_loss=  0.1530.\n",
      "NOTE: Epoch i=71, g_loss=  1.0431, d_loss=  0.0387.\n",
      "NOTE: Epoch i=72, g_loss=  1.1662, d_loss=  0.3773.\n",
      "NOTE: Epoch i=73, g_loss=  1.2605, d_loss=  0.0300.\n",
      "NOTE: Epoch i=74, g_loss=  1.6589, d_loss=  0.1314.\n",
      "NOTE: Epoch i=75, g_loss=  1.4911, d_loss= -0.0918.\n",
      "NOTE: Epoch i=76, g_loss=  1.5288, d_loss= -0.0843.\n",
      "NOTE: Epoch i=77, g_loss=  1.6732, d_loss= -0.1688.\n",
      "NOTE: Epoch i=78, g_loss=  1.6628, d_loss= -0.0557.\n",
      "NOTE: Epoch i=79, g_loss=  1.7737, d_loss= -0.2409.\n",
      "NOTE: Epoch i=80, g_loss=  1.9016, d_loss= -0.2980.\n",
      "NOTE: Epoch i=81, g_loss=  1.8683, d_loss= -0.6236.\n",
      "NOTE: Epoch i=82, g_loss=  1.9945, d_loss= -0.3536.\n",
      "NOTE: Epoch i=83, g_loss=  2.2586, d_loss= -0.5490.\n",
      "NOTE: Epoch i=84, g_loss=  2.2324, d_loss= -0.2853.\n",
      "NOTE: Epoch i=85, g_loss=  2.2515, d_loss= -0.4124.\n",
      "NOTE: Epoch i=86, g_loss=  2.2456, d_loss= -0.4721.\n",
      "NOTE: Epoch i=87, g_loss=  2.4004, d_loss= -0.5537.\n",
      "NOTE: Epoch i=88, g_loss=  2.3108, d_loss= -0.0867.\n",
      "NOTE: Epoch i=89, g_loss=  1.9971, d_loss= -0.2127.\n",
      "NOTE: Epoch i=90, g_loss=  2.1460, d_loss= -0.1753.\n",
      "NOTE: Epoch i=91, g_loss=  1.7634, d_loss= -0.2082.\n",
      "NOTE: Epoch i=92, g_loss=  1.7350, d_loss= -0.1849.\n",
      "NOTE: Epoch i=93, g_loss=  1.8872, d_loss= -0.0242.\n",
      "NOTE: Epoch i=94, g_loss=  1.7696, d_loss= -0.0753.\n",
      "NOTE: Epoch i=95, g_loss=  1.7258, d_loss=  0.0608.\n",
      "NOTE: Epoch i=96, g_loss=  1.5969, d_loss= -0.0196.\n",
      "NOTE: Epoch i=97, g_loss=  1.7843, d_loss= -0.1875.\n",
      "NOTE: Epoch i=98, g_loss=  1.6170, d_loss= -0.0961.\n",
      "NOTE: Epoch i=99, g_loss=  1.7629, d_loss= -0.0644.\n",
      "NOTE: Epoch i=100, g_loss=  1.6054, d_loss= -0.1093.\n",
      "NOTE: Epoch i=101, g_loss=  1.4426, d_loss= -0.0690.\n",
      "NOTE: Epoch i=102, g_loss=  1.2109, d_loss= -0.0914.\n",
      "NOTE: Epoch i=103, g_loss=  1.3075, d_loss= -0.0885.\n",
      "NOTE: Epoch i=104, g_loss=  1.0993, d_loss= -0.0898.\n",
      "NOTE: Epoch i=105, g_loss=  1.0657, d_loss=  0.0621.\n",
      "NOTE: Epoch i=106, g_loss=  1.0087, d_loss= -0.0859.\n",
      "NOTE: Epoch i=107, g_loss=  1.1432, d_loss= -0.3512.\n",
      "NOTE: Epoch i=108, g_loss=  1.2585, d_loss= -0.1712.\n",
      "NOTE: Epoch i=109, g_loss=  0.8932, d_loss= -0.1680.\n",
      "NOTE: Epoch i=110, g_loss=  0.8857, d_loss= -0.1814.\n",
      "NOTE: Epoch i=111, g_loss=  1.1179, d_loss= -0.0376.\n",
      "NOTE: Epoch i=112, g_loss=  1.1469, d_loss= -0.0571.\n",
      "NOTE: Epoch i=113, g_loss=  1.0723, d_loss=  0.0421.\n",
      "NOTE: Epoch i=114, g_loss=  1.1850, d_loss= -0.2471.\n",
      "NOTE: Epoch i=115, g_loss=  1.1268, d_loss= -0.1535.\n",
      "NOTE: Epoch i=116, g_loss=  1.1854, d_loss= -0.2677.\n",
      "NOTE: Epoch i=117, g_loss=  0.9960, d_loss=  0.1365.\n",
      "NOTE: Epoch i=118, g_loss=  0.9166, d_loss= -0.1888.\n",
      "NOTE: Epoch i=119, g_loss=  1.1726, d_loss= -0.2516.\n",
      "NOTE: Epoch i=120, g_loss=  1.1324, d_loss= -0.0338.\n",
      "NOTE: Epoch i=121, g_loss=  1.3168, d_loss=  0.1027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=122, g_loss=  1.2766, d_loss= -0.2144.\n",
      "NOTE: Epoch i=123, g_loss=  1.2138, d_loss= -0.2094.\n",
      "NOTE: Epoch i=124, g_loss=  1.3176, d_loss= -0.3577.\n",
      "NOTE: Epoch i=125, g_loss=  1.2997, d_loss=  0.0844.\n",
      "NOTE: Epoch i=126, g_loss=  1.3592, d_loss= -0.1173.\n",
      "NOTE: Epoch i=127, g_loss=  1.5433, d_loss=  0.0447.\n",
      "NOTE: Epoch i=128, g_loss=  1.5250, d_loss= -0.3129.\n",
      "NOTE: Epoch i=129, g_loss=  1.5101, d_loss= -0.4028.\n",
      "NOTE: Epoch i=130, g_loss=  1.5996, d_loss= -0.2293.\n",
      "NOTE: Epoch i=131, g_loss=  1.8096, d_loss= -0.3738.\n",
      "NOTE: Epoch i=132, g_loss=  1.8199, d_loss= -0.2460.\n",
      "NOTE: Epoch i=133, g_loss=  1.8249, d_loss= -0.3039.\n",
      "NOTE: Epoch i=134, g_loss=  1.8978, d_loss= -0.3696.\n",
      "NOTE: Epoch i=135, g_loss=  1.8327, d_loss= -0.0426.\n",
      "NOTE: Epoch i=136, g_loss=  1.8115, d_loss= -0.4307.\n",
      "NOTE: Epoch i=137, g_loss=  1.8783, d_loss=  0.1764.\n",
      "NOTE: Epoch i=138, g_loss=  1.7403, d_loss= -0.3118.\n",
      "NOTE: Epoch i=139, g_loss=  1.7309, d_loss= -0.1146.\n",
      "NOTE: Epoch i=140, g_loss=  1.7709, d_loss= -0.2321.\n",
      "NOTE: Epoch i=141, g_loss=  1.5487, d_loss= -0.0951.\n",
      "NOTE: Epoch i=142, g_loss=  1.5122, d_loss= -0.1217.\n",
      "NOTE: Epoch i=143, g_loss=  1.5144, d_loss= -0.0479.\n",
      "NOTE: Epoch i=144, g_loss=  1.5206, d_loss= -0.2914.\n",
      "NOTE: Epoch i=145, g_loss=  1.5224, d_loss= -0.3235.\n",
      "NOTE: Epoch i=146, g_loss=  1.4905, d_loss= -0.3531.\n",
      "NOTE: Epoch i=147, g_loss=  1.5105, d_loss= -0.2267.\n",
      "NOTE: Epoch i=148, g_loss=  1.3303, d_loss= -0.3419.\n",
      "NOTE: Epoch i=149, g_loss=  1.3514, d_loss=  0.0293.\n",
      "NOTE: Epoch i=150, g_loss=  1.5250, d_loss=  0.0087.\n",
      "NOTE: Epoch i=151, g_loss=  1.6402, d_loss= -0.2970.\n",
      "NOTE: Epoch i=152, g_loss=  1.6211, d_loss= -0.1836.\n",
      "NOTE: Epoch i=153, g_loss=  1.3844, d_loss= -0.1497.\n",
      "NOTE: Epoch i=154, g_loss=  1.5884, d_loss=  0.0228.\n",
      "NOTE: Epoch i=155, g_loss=  1.6466, d_loss= -0.2901.\n",
      "NOTE: Epoch i=156, g_loss=  1.6769, d_loss= -0.1865.\n",
      "NOTE: Epoch i=157, g_loss=  1.8642, d_loss= -0.2045.\n",
      "NOTE: Epoch i=158, g_loss=  1.5384, d_loss= -0.1364.\n",
      "NOTE: Epoch i=159, g_loss=  1.7070, d_loss= -0.1706.\n",
      "NOTE: Epoch i=160, g_loss=  1.8230, d_loss= -0.1942.\n",
      "NOTE: Epoch i=161, g_loss=  1.8011, d_loss= -0.3522.\n",
      "NOTE: Epoch i=162, g_loss=  1.5227, d_loss=  0.0383.\n",
      "NOTE: Epoch i=163, g_loss=  1.4759, d_loss= -0.3266.\n",
      "NOTE: Epoch i=164, g_loss=  1.5845, d_loss= -0.1440.\n",
      "NOTE: Epoch i=165, g_loss=  1.3701, d_loss= -0.0983.\n",
      "NOTE: Epoch i=166, g_loss=  1.3742, d_loss= -0.1705.\n",
      "NOTE: Epoch i=167, g_loss=  1.4843, d_loss= -0.2011.\n",
      "NOTE: Epoch i=168, g_loss=  1.5812, d_loss= -0.0467.\n",
      "NOTE: Epoch i=169, g_loss=  1.5299, d_loss= -0.1777.\n",
      "NOTE: Epoch i=170, g_loss=  1.2641, d_loss= -0.0081.\n",
      "NOTE: Epoch i=171, g_loss=  1.2698, d_loss= -0.3322.\n",
      "NOTE: Epoch i=172, g_loss=  1.6972, d_loss= -0.1056.\n",
      "NOTE: Epoch i=173, g_loss=  1.5647, d_loss= -0.1572.\n",
      "NOTE: Epoch i=174, g_loss=  1.6060, d_loss= -0.1391.\n",
      "NOTE: Epoch i=175, g_loss=  1.7038, d_loss= -0.3310.\n",
      "NOTE: Epoch i=176, g_loss=  1.5401, d_loss= -0.1049.\n",
      "NOTE: Epoch i=177, g_loss=  1.7047, d_loss= -0.0314.\n",
      "NOTE: Epoch i=178, g_loss=  1.6411, d_loss= -0.1418.\n",
      "NOTE: Epoch i=179, g_loss=  1.7901, d_loss= -0.3124.\n",
      "NOTE: Epoch i=180, g_loss=  1.5897, d_loss= -0.1634.\n",
      "NOTE: Epoch i=181, g_loss=  1.5083, d_loss= -0.0531.\n",
      "NOTE: Epoch i=182, g_loss=  1.4692, d_loss= -0.0476.\n",
      "NOTE: Epoch i=183, g_loss=  1.6703, d_loss= -0.1866.\n",
      "NOTE: Epoch i=184, g_loss=  1.5943, d_loss= -0.1311.\n",
      "NOTE: Epoch i=185, g_loss=  1.7321, d_loss= -0.0969.\n",
      "NOTE: Epoch i=186, g_loss=  1.7422, d_loss=  0.0471.\n",
      "NOTE: Epoch i=187, g_loss=  1.6700, d_loss= -0.0040.\n",
      "NOTE: Epoch i=188, g_loss=  1.7814, d_loss= -0.1072.\n",
      "NOTE: Epoch i=189, g_loss=  1.6826, d_loss= -0.2442.\n",
      "NOTE: Epoch i=190, g_loss=  1.8241, d_loss= -0.2197.\n",
      "NOTE: Epoch i=191, g_loss=  1.6181, d_loss= -0.0720.\n",
      "NOTE: Epoch i=192, g_loss=  1.8592, d_loss= -0.0019.\n",
      "NOTE: Epoch i=193, g_loss=  1.7956, d_loss= -0.2712.\n",
      "NOTE: Epoch i=194, g_loss=  1.7629, d_loss= -0.0144.\n",
      "NOTE: Epoch i=195, g_loss=  1.6528, d_loss= -0.0738.\n",
      "NOTE: Epoch i=196, g_loss=  1.6365, d_loss=  0.0049.\n",
      "NOTE: Epoch i=197, g_loss=  1.7578, d_loss= -0.0170.\n",
      "NOTE: Epoch i=198, g_loss=  1.5729, d_loss=  0.0531.\n",
      "NOTE: Epoch i=199, g_loss=  1.8914, d_loss= -0.2182.\n",
      "NOTE: Epoch i=200, g_loss=  1.6952, d_loss= -0.3669.\n",
      "NOTE: Epoch i=201, g_loss=  1.7256, d_loss= -0.4051.\n",
      "NOTE: Epoch i=202, g_loss=  1.8333, d_loss= -0.1131.\n",
      "NOTE: Epoch i=203, g_loss=  1.6027, d_loss= -0.1838.\n",
      "NOTE: Epoch i=204, g_loss=  1.8941, d_loss= -0.0327.\n",
      "NOTE: Epoch i=205, g_loss=  1.7457, d_loss= -0.2003.\n",
      "NOTE: Epoch i=206, g_loss=  1.8962, d_loss= -0.1694.\n",
      "NOTE: Epoch i=207, g_loss=  1.6857, d_loss= -0.2008.\n",
      "NOTE: Epoch i=208, g_loss=  1.6411, d_loss= -0.0200.\n",
      "NOTE: Epoch i=209, g_loss=  1.6947, d_loss= -0.1689.\n",
      "NOTE: Epoch i=210, g_loss=  1.6255, d_loss= -0.0910.\n",
      "NOTE: Epoch i=211, g_loss=  1.6952, d_loss= -0.2048.\n",
      "NOTE: Epoch i=212, g_loss=  1.8316, d_loss= -0.3274.\n",
      "NOTE: Epoch i=213, g_loss=  1.7209, d_loss= -0.4415.\n",
      "NOTE: Epoch i=214, g_loss=  1.7232, d_loss= -0.3882.\n",
      "NOTE: Epoch i=215, g_loss=  1.7585, d_loss= -0.1229.\n",
      "NOTE: Epoch i=216, g_loss=  1.7158, d_loss= -0.0452.\n",
      "NOTE: Epoch i=217, g_loss=  1.5881, d_loss= -0.1301.\n",
      "NOTE: Epoch i=218, g_loss=  1.5762, d_loss= -0.0683.\n",
      "NOTE: Epoch i=219, g_loss=  1.6637, d_loss= -0.0707.\n",
      "NOTE: Epoch i=220, g_loss=  1.7259, d_loss= -0.1781.\n",
      "NOTE: Epoch i=221, g_loss=  1.7811, d_loss= -0.1343.\n",
      "NOTE: Epoch i=222, g_loss=  1.6844, d_loss= -0.1325.\n",
      "NOTE: Epoch i=223, g_loss=  1.7374, d_loss= -0.1155.\n",
      "NOTE: Epoch i=224, g_loss=  1.7332, d_loss= -0.0434.\n",
      "NOTE: Epoch i=225, g_loss=  1.8297, d_loss= -0.2458.\n",
      "NOTE: Epoch i=226, g_loss=  1.8447, d_loss=  0.0490.\n",
      "NOTE: Epoch i=227, g_loss=  1.6443, d_loss= -0.4282.\n",
      "NOTE: Epoch i=228, g_loss=  1.6909, d_loss= -0.0565.\n",
      "NOTE: Epoch i=229, g_loss=  1.7865, d_loss= -0.1815.\n",
      "NOTE: Epoch i=230, g_loss=  1.7509, d_loss= -0.3198.\n",
      "NOTE: Epoch i=231, g_loss=  1.7472, d_loss= -0.3415.\n",
      "NOTE: Epoch i=232, g_loss=  1.6452, d_loss= -0.1958.\n",
      "NOTE: Epoch i=233, g_loss=  1.8393, d_loss= -0.2357.\n",
      "NOTE: Epoch i=234, g_loss=  1.7895, d_loss= -0.1874.\n",
      "NOTE: Epoch i=235, g_loss=  1.7814, d_loss= -0.1939.\n",
      "NOTE: Epoch i=236, g_loss=  1.8122, d_loss= -0.1721.\n",
      "NOTE: Epoch i=237, g_loss=  2.0655, d_loss= -0.2534.\n",
      "NOTE: Epoch i=238, g_loss=  1.8623, d_loss= -0.2361.\n",
      "NOTE: Epoch i=239, g_loss=  1.7884, d_loss=  0.2706.\n",
      "NOTE: Epoch i=240, g_loss=  1.8090, d_loss= -0.2320.\n",
      "NOTE: Epoch i=241, g_loss=  1.8603, d_loss=  0.0670.\n",
      "NOTE: Epoch i=242, g_loss=  1.7294, d_loss= -0.2427.\n",
      "NOTE: Epoch i=243, g_loss=  1.5938, d_loss= -0.2161.\n",
      "NOTE: Epoch i=244, g_loss=  1.9122, d_loss= -0.1763.\n",
      "NOTE: Epoch i=245, g_loss=  1.6864, d_loss= -0.2192.\n",
      "NOTE: Epoch i=246, g_loss=  1.7828, d_loss= -0.3272.\n",
      "NOTE: Epoch i=247, g_loss=  1.6479, d_loss= -0.2333.\n",
      "NOTE: Epoch i=248, g_loss=  1.6200, d_loss= -0.1825.\n",
      "NOTE: Epoch i=249, g_loss=  1.6330, d_loss= -0.2531.\n",
      "NOTE: Epoch i=250, g_loss=  1.7484, d_loss= -0.1209.\n",
      "NOTE: Epoch i=251, g_loss=  1.7108, d_loss= -0.0589.\n",
      "NOTE: Epoch i=252, g_loss=  1.5954, d_loss= -0.3891.\n",
      "NOTE: Epoch i=253, g_loss=  1.5626, d_loss= -0.1327.\n",
      "NOTE: Epoch i=254, g_loss=  1.6205, d_loss= -0.3590.\n",
      "NOTE: Epoch i=255, g_loss=  1.6096, d_loss= -0.3080.\n",
      "NOTE: Epoch i=256, g_loss=  1.5952, d_loss= -0.0832.\n",
      "NOTE: Epoch i=257, g_loss=  1.6770, d_loss=  0.2190.\n",
      "NOTE: Epoch i=258, g_loss=  1.9007, d_loss= -0.1302.\n",
      "NOTE: Epoch i=259, g_loss=  1.8613, d_loss= -0.0466.\n",
      "NOTE: Epoch i=260, g_loss=  1.6521, d_loss= -0.0376.\n",
      "NOTE: Epoch i=261, g_loss=  1.8922, d_loss= -0.4212.\n",
      "NOTE: Epoch i=262, g_loss=  1.7260, d_loss= -0.1308.\n",
      "NOTE: Epoch i=263, g_loss=  1.8975, d_loss= -0.1389.\n",
      "NOTE: Epoch i=264, g_loss=  1.6556, d_loss=  0.0304.\n",
      "NOTE: Epoch i=265, g_loss=  1.7797, d_loss= -0.1519.\n",
      "NOTE: Epoch i=266, g_loss=  1.8345, d_loss= -0.1443.\n",
      "NOTE: Epoch i=267, g_loss=  1.8846, d_loss= -0.1437.\n",
      "NOTE: Epoch i=268, g_loss=  1.8664, d_loss= -0.3066.\n",
      "NOTE: Epoch i=269, g_loss=  1.8760, d_loss= -0.0322.\n",
      "NOTE: Epoch i=270, g_loss=  1.6931, d_loss= -0.1815.\n",
      "NOTE: Epoch i=271, g_loss=  1.9164, d_loss= -0.1692.\n",
      "NOTE: Epoch i=272, g_loss=  1.9075, d_loss= -0.3021.\n",
      "NOTE: Epoch i=273, g_loss=  1.7667, d_loss= -0.1890.\n",
      "NOTE: Epoch i=274, g_loss=  1.7316, d_loss= -0.0945.\n",
      "NOTE: Epoch i=275, g_loss=  1.7666, d_loss= -0.1590.\n",
      "NOTE: Epoch i=276, g_loss=  1.7901, d_loss= -0.2481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=277, g_loss=  1.7944, d_loss= -0.2367.\n",
      "NOTE: Epoch i=278, g_loss=  1.7778, d_loss= -0.2681.\n",
      "NOTE: Epoch i=279, g_loss=  1.6813, d_loss= -0.3671.\n",
      "NOTE: Epoch i=280, g_loss=  1.6510, d_loss= -0.1540.\n",
      "NOTE: Epoch i=281, g_loss=  1.7481, d_loss=  0.0095.\n",
      "NOTE: Epoch i=282, g_loss=  1.8449, d_loss= -0.2867.\n",
      "NOTE: Epoch i=283, g_loss=  1.7259, d_loss= -0.1409.\n",
      "NOTE: Epoch i=284, g_loss=  1.9050, d_loss= -0.1794.\n",
      "NOTE: Epoch i=285, g_loss=  1.7343, d_loss= -0.2281.\n",
      "NOTE: Epoch i=286, g_loss=  1.9310, d_loss= -0.1722.\n",
      "NOTE: Epoch i=287, g_loss=  1.7879, d_loss= -0.1206.\n",
      "NOTE: Epoch i=288, g_loss=  1.7064, d_loss= -0.2995.\n",
      "NOTE: Epoch i=289, g_loss=  1.7288, d_loss=  0.0736.\n",
      "NOTE: Epoch i=290, g_loss=  1.7845, d_loss= -0.2008.\n",
      "NOTE: Epoch i=291, g_loss=  1.6559, d_loss= -0.1390.\n",
      "NOTE: Epoch i=292, g_loss=  1.8070, d_loss=  0.1205.\n",
      "NOTE: Epoch i=293, g_loss=  1.7935, d_loss= -0.0537.\n",
      "NOTE: Epoch i=294, g_loss=  1.9107, d_loss= -0.3011.\n",
      "NOTE: Epoch i=295, g_loss=  1.9135, d_loss= -0.1432.\n",
      "NOTE: Epoch i=296, g_loss=  1.9757, d_loss=  0.0050.\n",
      "NOTE: Epoch i=297, g_loss=  1.7320, d_loss= -0.2884.\n",
      "NOTE: Epoch i=298, g_loss=  1.6558, d_loss= -0.3105.\n",
      "NOTE: Epoch i=299, g_loss=  1.7708, d_loss= -0.0309.\n",
      "NOTE: Epoch i=300, g_loss=  1.8991, d_loss= -0.0010.\n",
      "NOTE: Epoch i=301, g_loss=  1.8365, d_loss= -0.3063.\n",
      "NOTE: Epoch i=302, g_loss=  1.8489, d_loss= -0.0701.\n",
      "NOTE: Epoch i=303, g_loss=  1.8393, d_loss= -0.2043.\n",
      "NOTE: Epoch i=304, g_loss=  1.7079, d_loss= -0.1235.\n",
      "NOTE: Epoch i=305, g_loss=  1.9527, d_loss= -0.2278.\n",
      "NOTE: Epoch i=306, g_loss=  1.7891, d_loss= -0.1139.\n",
      "NOTE: Epoch i=307, g_loss=  1.7586, d_loss= -0.0698.\n",
      "NOTE: Epoch i=308, g_loss=  1.9326, d_loss= -0.3271.\n",
      "NOTE: Epoch i=309, g_loss=  1.8836, d_loss= -0.1536.\n",
      "NOTE: Epoch i=310, g_loss=  1.8211, d_loss= -0.0253.\n",
      "NOTE: Epoch i=311, g_loss=  1.8066, d_loss= -0.2020.\n",
      "NOTE: Epoch i=312, g_loss=  1.6754, d_loss= -0.2524.\n",
      "NOTE: Epoch i=313, g_loss=  1.9620, d_loss= -0.2908.\n",
      "NOTE: Epoch i=314, g_loss=  1.8711, d_loss= -0.3223.\n",
      "NOTE: Epoch i=315, g_loss=  1.7119, d_loss= -0.0955.\n",
      "NOTE: Epoch i=316, g_loss=  1.8857, d_loss=  0.1154.\n",
      "NOTE: Epoch i=317, g_loss=  1.8530, d_loss= -0.0199.\n",
      "NOTE: Epoch i=318, g_loss=  1.9535, d_loss= -0.0521.\n",
      "NOTE: Epoch i=319, g_loss=  1.7576, d_loss= -0.0906.\n",
      "NOTE: Epoch i=320, g_loss=  1.8493, d_loss= -0.2075.\n",
      "NOTE: Epoch i=321, g_loss=  2.0043, d_loss= -0.0451.\n",
      "NOTE: Epoch i=322, g_loss=  1.8935, d_loss=  0.0655.\n",
      "NOTE: Epoch i=323, g_loss=  1.7434, d_loss= -0.1649.\n",
      "NOTE: Epoch i=324, g_loss=  1.9031, d_loss= -0.0933.\n",
      "NOTE: Epoch i=325, g_loss=  1.6881, d_loss= -0.2013.\n",
      "NOTE: Epoch i=326, g_loss=  2.1209, d_loss= -0.2412.\n",
      "NOTE: Epoch i=327, g_loss=  1.8109, d_loss=  0.0192.\n",
      "NOTE: Epoch i=328, g_loss=  1.8803, d_loss= -0.0291.\n",
      "NOTE: Epoch i=329, g_loss=  1.8385, d_loss= -0.1408.\n",
      "NOTE: Epoch i=330, g_loss=  1.8880, d_loss= -0.1529.\n",
      "NOTE: Epoch i=331, g_loss=  1.8127, d_loss= -0.2050.\n",
      "NOTE: Epoch i=332, g_loss=  1.8335, d_loss= -0.1985.\n",
      "NOTE: Epoch i=333, g_loss=  1.9214, d_loss= -0.2447.\n",
      "NOTE: Epoch i=334, g_loss=  2.0085, d_loss= -0.0365.\n",
      "NOTE: Epoch i=335, g_loss=  1.8198, d_loss= -0.1844.\n",
      "NOTE: Epoch i=336, g_loss=  1.7808, d_loss= -0.2994.\n",
      "NOTE: Epoch i=337, g_loss=  1.9603, d_loss= -0.2764.\n",
      "NOTE: Epoch i=338, g_loss=  1.8799, d_loss= -0.0067.\n",
      "NOTE: Epoch i=339, g_loss=  1.8961, d_loss=  0.0689.\n",
      "NOTE: Epoch i=340, g_loss=  1.7671, d_loss= -0.2887.\n",
      "NOTE: Epoch i=341, g_loss=  1.6394, d_loss= -0.0957.\n",
      "NOTE: Epoch i=342, g_loss=  1.6976, d_loss= -0.2693.\n",
      "NOTE: Epoch i=343, g_loss=  1.9545, d_loss= -0.1580.\n",
      "NOTE: Epoch i=344, g_loss=  1.8375, d_loss= -0.2275.\n",
      "NOTE: Epoch i=345, g_loss=  1.8485, d_loss= -0.4810.\n",
      "NOTE: Epoch i=346, g_loss=  1.9306, d_loss= -0.1164.\n",
      "NOTE: Epoch i=347, g_loss=  1.8697, d_loss= -0.1756.\n",
      "NOTE: Epoch i=348, g_loss=  1.8814, d_loss= -0.2549.\n",
      "NOTE: Epoch i=349, g_loss=  1.9507, d_loss= -0.1288.\n",
      "NOTE: Epoch i=350, g_loss=  1.8048, d_loss= -0.5440.\n",
      "NOTE: Epoch i=351, g_loss=  1.9631, d_loss= -0.2968.\n",
      "NOTE: Epoch i=352, g_loss=  1.9935, d_loss= -0.0684.\n",
      "NOTE: Epoch i=353, g_loss=  1.9605, d_loss= -0.2594.\n",
      "NOTE: Epoch i=354, g_loss=  1.9306, d_loss= -0.1058.\n",
      "NOTE: Epoch i=355, g_loss=  2.0126, d_loss= -0.4367.\n",
      "NOTE: Epoch i=356, g_loss=  1.9240, d_loss= -0.0630.\n",
      "NOTE: Epoch i=357, g_loss=  1.8778, d_loss= -0.2836.\n",
      "NOTE: Epoch i=358, g_loss=  1.9620, d_loss= -0.2220.\n",
      "NOTE: Epoch i=359, g_loss=  1.7467, d_loss= -0.1854.\n",
      "NOTE: Epoch i=360, g_loss=  1.7150, d_loss= -0.3706.\n",
      "NOTE: Epoch i=361, g_loss=  1.6237, d_loss= -0.2712.\n",
      "NOTE: Epoch i=362, g_loss=  1.5782, d_loss= -0.4147.\n",
      "NOTE: Epoch i=363, g_loss=  1.7543, d_loss= -0.1251.\n",
      "NOTE: Epoch i=364, g_loss=  1.6962, d_loss= -0.2225.\n",
      "NOTE: Epoch i=365, g_loss=  1.9742, d_loss=  0.0896.\n",
      "NOTE: Epoch i=366, g_loss=  1.7636, d_loss= -0.2925.\n",
      "NOTE: Epoch i=367, g_loss=  1.8915, d_loss= -0.1944.\n",
      "NOTE: Epoch i=368, g_loss=  1.6173, d_loss= -0.0392.\n",
      "NOTE: Epoch i=369, g_loss=  1.8913, d_loss= -0.1952.\n",
      "NOTE: Epoch i=370, g_loss=  1.9420, d_loss= -0.1545.\n",
      "NOTE: Epoch i=371, g_loss=  1.7401, d_loss= -0.4235.\n",
      "NOTE: Epoch i=372, g_loss=  1.8741, d_loss= -0.2247.\n",
      "NOTE: Epoch i=373, g_loss=  1.8327, d_loss= -0.4055.\n",
      "NOTE: Epoch i=374, g_loss=  1.8533, d_loss= -0.0310.\n",
      "NOTE: Epoch i=375, g_loss=  1.7369, d_loss= -0.4037.\n",
      "NOTE: Epoch i=376, g_loss=  1.7429, d_loss= -0.2336.\n",
      "NOTE: Epoch i=377, g_loss=  1.8930, d_loss= -0.1230.\n",
      "NOTE: Epoch i=378, g_loss=  1.9622, d_loss= -0.1112.\n",
      "NOTE: Epoch i=379, g_loss=  1.8603, d_loss= -0.1616.\n",
      "NOTE: Epoch i=380, g_loss=  1.6841, d_loss= -0.1521.\n",
      "NOTE: Epoch i=381, g_loss=  1.8389, d_loss= -0.0091.\n",
      "NOTE: Epoch i=382, g_loss=  1.8438, d_loss= -0.3766.\n",
      "NOTE: Epoch i=383, g_loss=  1.8438, d_loss= -0.1700.\n",
      "NOTE: Epoch i=384, g_loss=  1.9432, d_loss= -0.1471.\n",
      "NOTE: Epoch i=385, g_loss=  1.9043, d_loss= -0.2877.\n",
      "NOTE: Epoch i=386, g_loss=  1.7176, d_loss= -0.1108.\n",
      "NOTE: Epoch i=387, g_loss=  1.8227, d_loss= -0.1052.\n",
      "NOTE: Epoch i=388, g_loss=  1.8057, d_loss=  0.0172.\n",
      "NOTE: Epoch i=389, g_loss=  1.7754, d_loss= -0.3416.\n",
      "NOTE: Epoch i=390, g_loss=  2.0297, d_loss=  0.0457.\n",
      "NOTE: Epoch i=391, g_loss=  1.9641, d_loss= -0.0665.\n",
      "NOTE: Epoch i=392, g_loss=  1.9804, d_loss= -0.0029.\n",
      "NOTE: Epoch i=393, g_loss=  1.7617, d_loss= -0.3732.\n",
      "NOTE: Epoch i=394, g_loss=  1.9678, d_loss= -0.2208.\n",
      "NOTE: Epoch i=395, g_loss=  1.7669, d_loss= -0.0096.\n",
      "NOTE: Epoch i=396, g_loss=  1.9086, d_loss= -0.1549.\n",
      "NOTE: Epoch i=397, g_loss=  1.8969, d_loss= -0.2603.\n",
      "NOTE: Epoch i=398, g_loss=  2.0441, d_loss= -0.1534.\n",
      "NOTE: Epoch i=399, g_loss=  2.0414, d_loss= -0.1973.\n",
      "NOTE: Epoch i=400, g_loss=  1.8616, d_loss= -0.1590.\n",
      "NOTE: Epoch i=401, g_loss=  2.1787, d_loss= -0.3197.\n",
      "NOTE: Epoch i=402, g_loss=  2.1762, d_loss= -0.1492.\n",
      "NOTE: Epoch i=403, g_loss=  2.1236, d_loss= -0.0880.\n",
      "NOTE: Epoch i=404, g_loss=  1.9866, d_loss= -0.2253.\n",
      "NOTE: Epoch i=405, g_loss=  1.9114, d_loss= -0.3972.\n",
      "NOTE: Epoch i=406, g_loss=  1.8600, d_loss= -0.1247.\n",
      "NOTE: Epoch i=407, g_loss=  1.8836, d_loss= -0.3441.\n",
      "NOTE: Epoch i=408, g_loss=  1.9154, d_loss=  0.0803.\n",
      "NOTE: Epoch i=409, g_loss=  1.7527, d_loss=  0.1197.\n",
      "NOTE: Epoch i=410, g_loss=  1.9209, d_loss= -0.1332.\n",
      "NOTE: Epoch i=411, g_loss=  1.8993, d_loss= -0.0873.\n",
      "NOTE: Epoch i=412, g_loss=  1.9543, d_loss= -0.1693.\n",
      "NOTE: Epoch i=413, g_loss=  1.8920, d_loss=  0.1382.\n",
      "NOTE: Epoch i=414, g_loss=  1.7833, d_loss=  0.1002.\n",
      "NOTE: Epoch i=415, g_loss=  1.9550, d_loss=  0.0920.\n",
      "NOTE: Epoch i=416, g_loss=  1.6799, d_loss= -0.0764.\n",
      "NOTE: Epoch i=417, g_loss=  1.6755, d_loss= -0.0437.\n",
      "NOTE: Epoch i=418, g_loss=  1.8850, d_loss= -0.1166.\n",
      "NOTE: Epoch i=419, g_loss=  1.9828, d_loss= -0.2260.\n",
      "NOTE: Epoch i=420, g_loss=  1.8118, d_loss= -0.2019.\n",
      "NOTE: Epoch i=421, g_loss=  1.9558, d_loss=  0.1113.\n",
      "NOTE: Epoch i=422, g_loss=  1.6069, d_loss= -0.3514.\n",
      "NOTE: Epoch i=423, g_loss=  1.6436, d_loss=  0.1556.\n",
      "NOTE: Epoch i=424, g_loss=  1.8764, d_loss= -0.1966.\n",
      "NOTE: Epoch i=425, g_loss=  1.8648, d_loss=  0.1655.\n",
      "NOTE: Epoch i=426, g_loss=  1.8346, d_loss= -0.1985.\n",
      "NOTE: Epoch i=427, g_loss=  1.6568, d_loss= -0.3047.\n",
      "NOTE: Epoch i=428, g_loss=  1.8047, d_loss= -0.1550.\n",
      "NOTE: Epoch i=429, g_loss=  1.8078, d_loss= -0.3115.\n",
      "NOTE: Epoch i=430, g_loss=  1.6174, d_loss= -0.0281.\n",
      "NOTE: Epoch i=431, g_loss=  1.8934, d_loss= -0.1523.\n",
      "NOTE: Epoch i=432, g_loss=  2.0439, d_loss= -0.3377.\n",
      "NOTE: Epoch i=433, g_loss=  1.9104, d_loss= -0.3734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=434, g_loss=  1.9825, d_loss=  0.0590.\n",
      "NOTE: Epoch i=435, g_loss=  1.9473, d_loss= -0.2029.\n",
      "NOTE: Epoch i=436, g_loss=  1.6998, d_loss= -0.1455.\n",
      "NOTE: Epoch i=437, g_loss=  1.7704, d_loss= -0.2172.\n",
      "NOTE: Epoch i=438, g_loss=  1.6549, d_loss= -0.3884.\n",
      "NOTE: Epoch i=439, g_loss=  1.9134, d_loss= -0.1753.\n",
      "NOTE: Epoch i=440, g_loss=  1.7907, d_loss= -0.3894.\n",
      "NOTE: Epoch i=441, g_loss=  1.6237, d_loss= -0.1943.\n",
      "NOTE: Epoch i=442, g_loss=  1.7564, d_loss= -0.4102.\n",
      "NOTE: Epoch i=443, g_loss=  1.6463, d_loss= -0.2092.\n",
      "NOTE: Epoch i=444, g_loss=  1.9220, d_loss= -0.3051.\n",
      "NOTE: Epoch i=445, g_loss=  1.8048, d_loss= -0.4229.\n",
      "NOTE: Epoch i=446, g_loss=  1.9727, d_loss= -0.2014.\n",
      "NOTE: Epoch i=447, g_loss=  1.6508, d_loss= -0.0470.\n",
      "NOTE: Epoch i=448, g_loss=  1.5756, d_loss= -0.1820.\n",
      "NOTE: Epoch i=449, g_loss=  1.9957, d_loss= -0.1470.\n",
      "NOTE: Epoch i=450, g_loss=  2.0505, d_loss= -0.0944.\n",
      "NOTE: Epoch i=451, g_loss=  1.8282, d_loss= -0.3154.\n",
      "NOTE: Epoch i=452, g_loss=  2.1114, d_loss= -0.0162.\n",
      "NOTE: Epoch i=453, g_loss=  1.7917, d_loss= -0.2760.\n",
      "NOTE: Epoch i=454, g_loss=  1.7840, d_loss= -0.2120.\n",
      "NOTE: Epoch i=455, g_loss=  1.9127, d_loss= -0.0326.\n",
      "NOTE: Epoch i=456, g_loss=  1.7948, d_loss= -0.1501.\n",
      "NOTE: Epoch i=457, g_loss=  2.0753, d_loss= -0.0951.\n",
      "NOTE: Epoch i=458, g_loss=  1.8970, d_loss= -0.1099.\n",
      "NOTE: Epoch i=459, g_loss=  1.8627, d_loss= -0.1215.\n",
      "NOTE: Epoch i=460, g_loss=  1.8039, d_loss= -0.1688.\n",
      "NOTE: Epoch i=461, g_loss=  1.8515, d_loss= -0.4663.\n",
      "NOTE: Epoch i=462, g_loss=  1.7849, d_loss= -0.0411.\n",
      "NOTE: Epoch i=463, g_loss=  1.8787, d_loss=  0.0178.\n",
      "NOTE: Epoch i=464, g_loss=  1.8446, d_loss= -0.2036.\n",
      "NOTE: Epoch i=465, g_loss=  1.8197, d_loss= -0.1305.\n",
      "NOTE: Epoch i=466, g_loss=  2.0351, d_loss= -0.1986.\n",
      "NOTE: Epoch i=467, g_loss=  2.0532, d_loss= -0.2692.\n",
      "NOTE: Epoch i=468, g_loss=  1.8832, d_loss= -0.3328.\n",
      "NOTE: Epoch i=469, g_loss=  1.9772, d_loss= -0.2464.\n",
      "NOTE: Epoch i=470, g_loss=  1.8415, d_loss= -0.3683.\n",
      "NOTE: Epoch i=471, g_loss=  1.7363, d_loss= -0.1891.\n",
      "NOTE: Epoch i=472, g_loss=  1.7770, d_loss= -0.3423.\n",
      "NOTE: Epoch i=473, g_loss=  1.8025, d_loss= -0.4021.\n",
      "NOTE: Epoch i=474, g_loss=  1.8525, d_loss= -0.1268.\n",
      "NOTE: Epoch i=475, g_loss=  1.8210, d_loss= -0.3210.\n",
      "NOTE: Epoch i=476, g_loss=  1.8938, d_loss= -0.5437.\n",
      "NOTE: Epoch i=477, g_loss=  1.8397, d_loss= -0.2857.\n",
      "NOTE: Epoch i=478, g_loss=  2.0323, d_loss= -0.1477.\n",
      "NOTE: Epoch i=479, g_loss=  2.0571, d_loss= -0.1041.\n",
      "NOTE: Epoch i=480, g_loss=  2.0187, d_loss= -0.1581.\n",
      "NOTE: Epoch i=481, g_loss=  1.8173, d_loss= -0.0367.\n",
      "NOTE: Epoch i=482, g_loss=  1.9784, d_loss= -0.3017.\n",
      "NOTE: Epoch i=483, g_loss=  2.0402, d_loss= -0.2875.\n",
      "NOTE: Epoch i=484, g_loss=  1.7904, d_loss= -0.3166.\n",
      "NOTE: Epoch i=485, g_loss=  2.1059, d_loss= -0.1462.\n",
      "NOTE: Epoch i=486, g_loss=  1.9947, d_loss= -0.2127.\n",
      "NOTE: Epoch i=487, g_loss=  1.9119, d_loss= -0.0909.\n",
      "NOTE: Epoch i=488, g_loss=  1.7945, d_loss= -0.0055.\n",
      "NOTE: Epoch i=489, g_loss=  2.0094, d_loss= -0.0061.\n",
      "NOTE: Epoch i=490, g_loss=  1.9295, d_loss= -0.1947.\n",
      "NOTE: Epoch i=491, g_loss=  1.8053, d_loss= -0.0310.\n",
      "NOTE: Epoch i=492, g_loss=  1.9124, d_loss= -0.1097.\n",
      "NOTE: Epoch i=493, g_loss=  1.9237, d_loss= -0.1522.\n",
      "NOTE: Epoch i=494, g_loss=  1.8915, d_loss= -0.2031.\n",
      "NOTE: Epoch i=495, g_loss=  2.0806, d_loss= -0.1920.\n",
      "NOTE: Epoch i=496, g_loss=  1.7845, d_loss= -0.5204.\n",
      "NOTE: Epoch i=497, g_loss=  1.8253, d_loss= -0.2707.\n",
      "NOTE: Epoch i=498, g_loss=  1.8736, d_loss= -0.5322.\n",
      "NOTE: Epoch i=499, g_loss=  1.8898, d_loss= -0.2069.\n",
      "NOTE: Epoch i=500, g_loss=  2.0592, d_loss= -0.1690.\n",
      "NOTE: 13184873 bytes were written to the table \"cpctStore\" in the caslib \"CASUSER(alphel)\".\n",
      "NOTE: tabularGanTrain action completed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; IterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch Number\">EpochNumber</th>\n",
       "      <th title=\"Autoencoder Loss\">AutoencoderLoss</th>\n",
       "      <th title=\"Generator Loss\">GeneratorLoss</th>\n",
       "      <th title=\"Discriminator Loss\">DiscriminatorLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.051210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.039171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.033617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.028943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.026629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.784547</td>\n",
       "      <td>-0.520382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.825257</td>\n",
       "      <td>-0.270682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.873594</td>\n",
       "      <td>-0.532154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.889817</td>\n",
       "      <td>-0.206895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.059191</td>\n",
       "      <td>-0.169006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; LevelFreq</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Variable Name\">VarName</th>\n",
       "      <th title=\"Level\">Level</th>\n",
       "      <th title=\"Frequency\">Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>education</td>\n",
       "      <td>10th</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>education</td>\n",
       "      <td>11th</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>education</td>\n",
       "      <td>12th</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>education</td>\n",
       "      <td>1st-4th</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>education</td>\n",
       "      <td>5th-6th</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>workclass</td>\n",
       "      <td>Private</td>\n",
       "      <td>25632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>workclass</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>workclass</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>workclass</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>workclass</td>\n",
       "      <td>Without-pay</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 3 columns</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"RowId\">RowId</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "      <th title=\"Numeric Value\">nValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMBEDDINGDIM</td>\n",
       "      <td>Generator Embedding Dimension</td>\n",
       "      <td>128</td>\n",
       "      <td>1.280000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MINIBATCHSIZE</td>\n",
       "      <td>Number of Observations in One Minibatch</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PACKSIZE</td>\n",
       "      <td>Number of Observations Group Together in Apply...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REGDWEIGHT</td>\n",
       "      <td>Weight for Regularizing the Discriminator</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPTIMIZERAE_BETA1</td>\n",
       "      <td>Exponential Decay Rate for the First Moment Es...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OPTIMIZERAE_BETA2</td>\n",
       "      <td>Exponential Decay Rate for the Second Moment E...</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>9.990000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OPTIMIZERAE_LEARNINGRATE</td>\n",
       "      <td>Learning Rate for the Autoencoder's Optimizer</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OPTIMIZERAE_NUMEPOCHS</td>\n",
       "      <td>Number of Epochs for the Autoencoder's Training</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OPTIMIZERAE_WEIGHTDECAY</td>\n",
       "      <td>Weight Decay for the Autoencoder's Optimizer</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1.000000e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OPTIMIZERGAN_BETA1</td>\n",
       "      <td>Exponential Decay Rate for the First Moment Es...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OPTIMIZERGAN_BETA2</td>\n",
       "      <td>Exponential Decay Rate for the Second Moment E...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OPTIMIZERGAN_LEARNINGRATE</td>\n",
       "      <td>Learning Rate for the GAN Optimizer</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OPTIMIZERGAN_NUMEPOCHS</td>\n",
       "      <td>Number of Epochs for the GAN Training</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OPTIMIZERGAN_WEIGHTDECAYD</td>\n",
       "      <td>Weight Decay for the Generator's Optimizer</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OPTIMIZERGAN_WEIGHTDECAYG</td>\n",
       "      <td>Weight Decay for the Discriminator's Optimizer</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SEED</td>\n",
       "      <td>Seed for Random Initialization</td>\n",
       "      <td>12345</td>\n",
       "      <td>1.234500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>USELOGLEVELFREQ</td>\n",
       "      <td>Whether to Use Log Frequency of Categorical Le...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; NObs</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"RowId\">RowId</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NREAD</td>\n",
       "      <td>Number of Observations Read</td>\n",
       "      <td>34189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUSED</td>\n",
       "      <td>Number of Observations Used</td>\n",
       "      <td>34189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(alphel)</td>\n",
       "      <td>out</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>CASTable('out', caslib='CASUSER(alphel)')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 2.37e+03s</span> &#183; <span class=\"cas-user\">user 1.03e+04s</span> &#183; <span class=\"cas-sys\">sys 427s</span> &#183; <span class=\"cas-memory\">mem 29MB</span></small></p>"
      ],
      "text/plain": [
       "[IterHistory]\n",
       "\n",
       "      EpochNumber  AutoencoderLoss  GeneratorLoss  DiscriminatorLoss\n",
       " 0              1         0.051210            NaN                NaN\n",
       " 1              2         0.039171            NaN                NaN\n",
       " 2              3         0.033617            NaN                NaN\n",
       " 3              4         0.028943            NaN                NaN\n",
       " 4              5         0.026629            NaN                NaN\n",
       " ..           ...              ...            ...                ...\n",
       " 995          496              NaN       1.784547          -0.520382\n",
       " 996          497              NaN       1.825257          -0.270682\n",
       " 997          498              NaN       1.873594          -0.532154\n",
       " 998          499              NaN       1.889817          -0.206895\n",
       " 999          500              NaN       2.059191          -0.169006\n",
       " \n",
       " [1000 rows x 4 columns]\n",
       "\n",
       "[LevelFreq]\n",
       "\n",
       "        VarName             Level  Frequency\n",
       " 0    education              10th        962\n",
       " 1    education              11th       1290\n",
       " 2    education              12th        462\n",
       " 3    education           1st-4th        168\n",
       " 4    education           5th-6th        363\n",
       " ..         ...               ...        ...\n",
       " 96   workclass           Private      25632\n",
       " 97   workclass      Self-emp-inc       1226\n",
       " 98   workclass  Self-emp-not-inc       2667\n",
       " 99   workclass         State-gov       1420\n",
       " 100  workclass       Without-pay         14\n",
       " \n",
       " [101 rows x 3 columns]\n",
       "\n",
       "[ModelInfo]\n",
       "\n",
       "                         RowId  \\\n",
       " 0                EMBEDDINGDIM   \n",
       " 1               MINIBATCHSIZE   \n",
       " 2                    PACKSIZE   \n",
       " 3                  REGDWEIGHT   \n",
       " 4           OPTIMIZERAE_BETA1   \n",
       " 5           OPTIMIZERAE_BETA2   \n",
       " 6    OPTIMIZERAE_LEARNINGRATE   \n",
       " 7       OPTIMIZERAE_NUMEPOCHS   \n",
       " 8     OPTIMIZERAE_WEIGHTDECAY   \n",
       " 9          OPTIMIZERGAN_BETA1   \n",
       " 10         OPTIMIZERGAN_BETA2   \n",
       " 11  OPTIMIZERGAN_LEARNINGRATE   \n",
       " 12     OPTIMIZERGAN_NUMEPOCHS   \n",
       " 13  OPTIMIZERGAN_WEIGHTDECAYD   \n",
       " 14  OPTIMIZERGAN_WEIGHTDECAYG   \n",
       " 15                       SEED   \n",
       " 16            USELOGLEVELFREQ   \n",
       " \n",
       "                                           Description     Value        nValue  \n",
       " 0                       Generator Embedding Dimension       128  1.280000e+02  \n",
       " 1             Number of Observations in One Minibatch       500  5.000000e+02  \n",
       " 2   Number of Observations Group Together in Apply...        10  1.000000e+01  \n",
       " 3           Weight for Regularizing the Discriminator        10  1.000000e+01  \n",
       " 4   Exponential Decay Rate for the First Moment Es...  0.900000  9.000000e-01  \n",
       " 5   Exponential Decay Rate for the Second Moment E...  0.999000  9.990000e-01  \n",
       " 6       Learning Rate for the Autoencoder's Optimizer     0.001  1.000000e-03  \n",
       " 7     Number of Epochs for the Autoencoder's Training       500  5.000000e+02  \n",
       " 8        Weight Decay for the Autoencoder's Optimizer     1e-08  1.000000e-08  \n",
       " 9   Exponential Decay Rate for the First Moment Es...  0.500000  5.000000e-01  \n",
       " 10  Exponential Decay Rate for the Second Moment E...  0.900000  9.000000e-01  \n",
       " 11                Learning Rate for the GAN Optimizer     2e-05  2.000000e-05  \n",
       " 12              Number of Epochs for the GAN Training       500  5.000000e+02  \n",
       " 13         Weight Decay for the Generator's Optimizer    0.0001  1.000000e-04  \n",
       " 14     Weight Decay for the Discriminator's Optimizer     1e-06  1.000000e-06  \n",
       " 15                     Seed for Random Initialization     12345  1.234500e+04  \n",
       " 16  Whether to Use Log Frequency of Categorical Le...      True  1.000000e+00  \n",
       "\n",
       "[NObs]\n",
       "\n",
       "    RowId                  Description  Value\n",
       " 0  NREAD  Number of Observations Read  34189\n",
       " 1  NUSED  Number of Observations Used  34189\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib Name Label  Rows  Columns  \\\n",
       " 0  CASUSER(alphel)  out           0       15   \n",
       " \n",
       "                                     casTable  \n",
       " 0  CASTable('out', caslib='CASUSER(alphel)')  \n",
       "\n",
       "+ Elapsed: 2.37e+03s, user: 1.03e+04s, sys: 427s, mem: 29mb"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = s.tabularGanTrain(\n",
    "table = {\"name\":\"GAN_data_train\"},\n",
    "    centroidsTable= \"cen\",\n",
    "    gpu = 1,\n",
    "    nominals = adult_discrete_columns,\n",
    "    optimizerAe ={\"method\":'ADAM',\"numEpochs\":500},\n",
    "    optimizerGan ={\"method\":'ADAM',\"numEpochs\":500},\n",
    "    seed = 12345,\n",
    "    scoreSeed = 1234,\n",
    "    numSamples =50000,\n",
    "    saveState ={\"name\":\"cpctStore\", \"replace\":True},\n",
    "    casOut = {\"name\":\"out\", \"replace\":True}\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table SCORE_TABLE in caslib CASUSER(alphel).\n",
      "NOTE: The table SCORE_TABLE has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    }
   ],
   "source": [
    "d = {'obs_num': range(1,50000)}\n",
    "df = pd.DataFrame(data=d)\n",
    "score_table = s.upload(df, casout='score_table').casTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'astore'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(alphel)</td>\n",
       "      <td>scoredData</td>\n",
       "      <td>49999</td>\n",
       "      <td>15</td>\n",
       "      <td>CASTable('scoredData', caslib='CASUSER(alphel)')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; Timing</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Task Timing</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Task\">Task</th>\n",
       "      <th title=\"Seconds\">Seconds</th>\n",
       "      <th title=\"Percent\">Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Loading the Store</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creating the State</td>\n",
       "      <td>0.156887</td>\n",
       "      <td>0.000810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scoring</td>\n",
       "      <td>193.620614</td>\n",
       "      <td>0.999132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total</td>\n",
       "      <td>193.788861</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 194s</span> &#183; <span class=\"cas-user\">user 193s</span> &#183; <span class=\"cas-sys\">sys 0.0236s</span> &#183; <span class=\"cas-memory\">mem 29.1MB</span></small></p>"
      ],
      "text/plain": [
       "[OutputCasTables]\n",
       "\n",
       "             casLib        Name   Rows  Columns  \\\n",
       " 0  CASUSER(alphel)  scoredData  49999       15   \n",
       " \n",
       "                                            casTable  \n",
       " 0  CASTable('scoredData', caslib='CASUSER(alphel)')  \n",
       "\n",
       "[Timing]\n",
       "\n",
       " Task Timing\n",
       " \n",
       "                  Task     Seconds   Percent\n",
       " 0   Loading the Store    0.010759  0.000056\n",
       " 1  Creating the State    0.156887  0.000810\n",
       " 2             Scoring  193.620614  0.999132\n",
       " 3               Total  193.788861  1.000000\n",
       "\n",
       "+ Elapsed: 194s, user: 193s, sys: 0.0236s, mem: 29.1mb"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loadactionset('astore')\n",
    "s.score(\n",
    "    table = {\"name\":\"score_table\"},\n",
    "    rstore = {'name':'cpctStore'},\n",
    "    casout = {'name':'scoredData','replace':True},\n",
    "    options =[{'name' :\"useGPU\",'value':1}],\n",
    "    nThreads = 1\n",
    "    #,_debug=dict(ranks='0', display='d7b308.na.sas.com:0', debugger='/u/xijian/bin/start-gdb-n06')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = s.fetch('scoredData', to=400000, maxrows=400000)['Fetch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    34994\n",
       "1.0    15005\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25989\n",
       "1     8200\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAN_data_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>education_10th</th>\n",
       "      <th>education_11th</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass_Federal-gov  workclass_Local-gov  workclass_Never-worked  \\\n",
       "0                      1                    0                       0   \n",
       "1                      0                    0                       0   \n",
       "2                      0                    0                       0   \n",
       "3                      0                    1                       0   \n",
       "4                      0                    0                       0   \n",
       "\n",
       "   workclass_Private  workclass_Self-emp-inc  workclass_Self-emp-not-inc  \\\n",
       "0                  0                       0                           0   \n",
       "1                  1                       0                           0   \n",
       "2                  0                       0                           1   \n",
       "3                  0                       0                           0   \n",
       "4                  1                       0                           0   \n",
       "\n",
       "   workclass_State-gov  workclass_Without-pay  education_10th  education_11th  \\\n",
       "0                    0                      0               0               0   \n",
       "1                    0                      0               0               0   \n",
       "2                    0                      0               0               0   \n",
       "3                    0                      0               0               0   \n",
       "4                    0                      0               0               0   \n",
       "\n",
       "   ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0  ...                        0                           0   \n",
       "1  ...                        0                           0   \n",
       "2  ...                        0                           0   \n",
       "3  ...                        0                           0   \n",
       "4  ...                        0                           0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        0                     0                      0   \n",
       "1                        0                     0                      0   \n",
       "2                        0                     0                      0   \n",
       "3                        0                     0                      0   \n",
       "4                        0                     0                      0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        0                               0   \n",
       "1                        0                               0   \n",
       "2                        0                               0   \n",
       "3                        0                               0   \n",
       "4                        0                               0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                             0                       0   \n",
       "1                             1                       0   \n",
       "2                             0                       0   \n",
       "3                             1                       0   \n",
       "4                             1                       0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the Ruiwen's Encoder for ML utility and CTGAN transformer for GAN\n",
    "str_cols= [ 'workclass', 'education', 'marital-status', 'relationship','race', 'sex','native-country']\n",
    "num_cols = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week', 'label']\n",
    "dataframe = pd.DataFrame(samples.loc[:,str_cols])\n",
    "\n",
    "one_hot_columns = pd.DataFrame()\n",
    "for col_name, item in dataframe.iteritems():\n",
    "    \n",
    "    #print(col_name)\n",
    "    #print(item)\n",
    "    col = pd.get_dummies(item, prefix=col_name)\n",
    "    one_hot_columns =pd.concat([one_hot_columns,col],axis=1)\n",
    "one_hot_columns.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire data is concat of discrete and contiuous cols\n",
    "fake_all = pd.concat([one_hot_columns,samples.loc[:,num_cols]],axis=1)\n",
    "#adult_data_all.head()\n",
    "fake_X = fake_all.drop([\"label\"],axis=1)\n",
    "fake_X, fake_y = fake_X,fake_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure cols of generated data has the same index sort \n",
    "fake_X = fake_X.reindex(adult_data_all.columns,axis=1, fill_value=0) # fill new cols with 0\n",
    "#len(fake_X.columns)\n",
    "fake_X = fake_X.drop([\"label\"],axis=1) # separate X and Y for training\n",
    "fake_X, fake_y = fake_X,fake_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the GAN generated train data:  (49999, 91)\n",
      "shape of the GAN generated labels:  (49999,)\n",
      "shape of the original test data:  (14653, 91)\n",
      "number of events in the fake data:  15005.0\n"
     ]
    }
   ],
   "source": [
    "# check the training data shape to agree with the original data (# of features/cols)\n",
    "print('shape of the GAN generated train data: ',fake_X.shape)\n",
    "print('shape of the GAN generated labels: ',fake_y.shape)\n",
    "print('shape of the original test data: ',orig_X_test.shape)\n",
    "print('number of events in the fake data: ', sum(fake_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Selected Rows from Table SCOREDDATA</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"age\">age</th>\n",
       "      <th title=\"workclass\">workclass</th>\n",
       "      <th title=\"fnlwgt\">fnlwgt</th>\n",
       "      <th title=\"education\">education</th>\n",
       "      <th title=\"education-num\">education-num</th>\n",
       "      <th title=\"marital-status\">marital-status</th>\n",
       "      <th title=\"occupation\">occupation</th>\n",
       "      <th title=\"relationship\">relationship</th>\n",
       "      <th title=\"race\">race</th>\n",
       "      <th title=\"sex\">sex</th>\n",
       "      <th title=\"capital-gain\">capital-gain</th>\n",
       "      <th title=\"capital-loss\">capital-loss</th>\n",
       "      <th title=\"hours-per-week\">hours-per-week</th>\n",
       "      <th title=\"native-country\">native-country</th>\n",
       "      <th title=\"label\">label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.737891</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>182737.746740</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>9.234240</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>53.031476</td>\n",
       "      <td>1.131567</td>\n",
       "      <td>39.922581</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.666563</td>\n",
       "      <td>Private</td>\n",
       "      <td>49777.611542</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.282768</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>39.462392</td>\n",
       "      <td>-1.303083</td>\n",
       "      <td>40.048517</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.605673</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>245199.502485</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8.834257</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>40.447554</td>\n",
       "      <td>0.073487</td>\n",
       "      <td>39.891228</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.370133</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>103169.833256</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>12.211463</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>17.671562</td>\n",
       "      <td>1.549371</td>\n",
       "      <td>40.162177</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.512905</td>\n",
       "      <td>Private</td>\n",
       "      <td>68645.649906</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>10.363951</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>-56.077158</td>\n",
       "      <td>0.064056</td>\n",
       "      <td>40.004266</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>21.278875</td>\n",
       "      <td>Private</td>\n",
       "      <td>168007.900398</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>1.242397</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>57.503529</td>\n",
       "      <td>-0.436013</td>\n",
       "      <td>11.509839</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>15.723144</td>\n",
       "      <td>Private</td>\n",
       "      <td>230638.361741</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>9.681814</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>41.741486</td>\n",
       "      <td>-0.634436</td>\n",
       "      <td>40.079225</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>35.294194</td>\n",
       "      <td>Private</td>\n",
       "      <td>106130.019572</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>5.565243</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>46.230484</td>\n",
       "      <td>0.680894</td>\n",
       "      <td>13.175063</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>26.668953</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>228116.189609</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8.345924</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>73.469619</td>\n",
       "      <td>-0.929161</td>\n",
       "      <td>40.008258</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>23.269561</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>2354.842516</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>10.148075</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>13.862745</td>\n",
       "      <td>1.472683</td>\n",
       "      <td>40.046700</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49999 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Selected Rows from Table SCOREDDATA\n",
       "\n",
       "             age         workclass         fnlwgt     education  \\\n",
       "0      26.737891       Federal-gov  182737.746740  Some-college   \n",
       "1      47.666563           Private   49777.611542       HS-grad   \n",
       "2      68.605673  Self-emp-not-inc  245199.502485       HS-grad   \n",
       "3      26.370133         Local-gov  103169.833256     Bachelors   \n",
       "4      25.512905           Private   68645.649906     Bachelors   \n",
       "...          ...               ...            ...           ...   \n",
       "49994  21.278875           Private  168007.900398       7th-8th   \n",
       "49995  15.723144           Private  230638.361741     Bachelors   \n",
       "49996  35.294194           Private  106130.019572       HS-grad   \n",
       "49997  26.668953       Federal-gov  228116.189609       HS-grad   \n",
       "49998  23.269561         Local-gov    2354.842516       HS-grad   \n",
       "\n",
       "       education-num      marital-status       occupation   relationship  \\\n",
       "0           9.234240  Married-civ-spouse     Craft-repair        Husband   \n",
       "1           9.282768       Never-married     Adm-clerical      Unmarried   \n",
       "2           8.834257  Married-civ-spouse  Farming-fishing        Husband   \n",
       "3          12.211463            Divorced  Farming-fishing      Unmarried   \n",
       "4          10.363951            Divorced   Prof-specialty           Wife   \n",
       "...              ...                 ...              ...            ...   \n",
       "49994       1.242397       Never-married            Sales  Not-in-family   \n",
       "49995       9.681814       Never-married   Prof-specialty  Not-in-family   \n",
       "49996       5.565243  Married-civ-spouse     Adm-clerical      Own-child   \n",
       "49997       8.345924       Never-married     Adm-clerical  Not-in-family   \n",
       "49998      10.148075  Married-civ-spouse    Other-service        Husband   \n",
       "\n",
       "        race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0      White    Male     53.031476      1.131567       39.922581   \n",
       "1      Black  Female     39.462392     -1.303083       40.048517   \n",
       "2      White    Male     40.447554      0.073487       39.891228   \n",
       "3      White  Female     17.671562      1.549371       40.162177   \n",
       "4      White  Female    -56.077158      0.064056       40.004266   \n",
       "...      ...     ...           ...           ...             ...   \n",
       "49994  White    Male     57.503529     -0.436013       11.509839   \n",
       "49995  White  Female     41.741486     -0.634436       40.079225   \n",
       "49996  White  Female     46.230484      0.680894       13.175063   \n",
       "49997  Black  Female     73.469619     -0.929161       40.008258   \n",
       "49998  White  Female     13.862745      1.472683       40.046700   \n",
       "\n",
       "      native-country  label  \n",
       "0           Cambodia    1.0  \n",
       "1      United-States    0.0  \n",
       "2           Cambodia    0.0  \n",
       "3      United-States    0.0  \n",
       "4      United-States    0.0  \n",
       "...              ...    ...  \n",
       "49994  United-States    0.0  \n",
       "49995  United-States    0.0  \n",
       "49996  United-States    0.0  \n",
       "49997  United-States    0.0  \n",
       "49998  United-States    0.0  \n",
       "\n",
       "[49999 rows x 15 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the GAN generated data:\n",
      "Decision Tree Acc:  0.7981300757524057 f-1:  0.5672908133411352 AUC: 0.772233293426658\n",
      "Linear SVM Acc:  0.3227325462362656 f-1:  0.3720577069096431 AUC: 0.5101207262963519\n",
      "Random Forest Acc:  0.7792260970449737 f-1:  0.5336600836096296 AUC: 0.8118015786071866\n",
      "Logistic Regression Acc:  0.7620282535999454 f-1:  0.0 AUC: 0.2818031776479882\n",
      "MLP Acc:  0.7613457994949839 f-1:  0.014652014652014654 AUC: 0.48317812158781615\n"
     ]
    }
   ],
   "source": [
    "# train a classifier on the CPCTGAN generated data\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]\n",
    "\n",
    "print('ML scores for the GAN generated data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(fake_X, fake_y)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    41434\n",
       "1.0     8566\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11166\n",
       "1     3487\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CTGAN on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G:  1.5204,Loss D: -0.0589\n",
      "Epoch 2, Loss G:  1.3082,Loss D: -0.0196\n",
      "Epoch 3, Loss G:  0.3795,Loss D: -0.0636\n",
      "Epoch 4, Loss G:  0.0595,Loss D: -0.0625\n",
      "Epoch 5, Loss G: -0.4881,Loss D:  0.0530\n",
      "Epoch 6, Loss G: -0.8528,Loss D:  0.1767\n",
      "Epoch 7, Loss G: -0.8646,Loss D: -0.0188\n",
      "Epoch 8, Loss G: -1.3634,Loss D:  0.1349\n",
      "Epoch 9, Loss G: -1.6166,Loss D: -0.1497\n",
      "Epoch 10, Loss G: -1.6396,Loss D: -0.0809\n",
      "Epoch 11, Loss G: -1.6600,Loss D:  0.0691\n",
      "Epoch 12, Loss G: -1.5443,Loss D: -0.0439\n",
      "Epoch 13, Loss G: -1.5796,Loss D:  0.0283\n",
      "Epoch 14, Loss G: -1.7363,Loss D: -0.0391\n",
      "Epoch 15, Loss G: -1.8126,Loss D: -0.0799\n",
      "Epoch 16, Loss G: -1.7884,Loss D: -0.1437\n",
      "Epoch 17, Loss G: -1.5641,Loss D: -0.2686\n",
      "Epoch 18, Loss G: -1.3873,Loss D: -0.1432\n",
      "Epoch 19, Loss G: -1.5918,Loss D: -0.2049\n",
      "Epoch 20, Loss G: -1.2530,Loss D: -0.1029\n",
      "Epoch 21, Loss G: -1.2472,Loss D: -0.0676\n",
      "Epoch 22, Loss G: -1.1067,Loss D: -0.0157\n",
      "Epoch 23, Loss G: -0.8825,Loss D:  0.1365\n",
      "Epoch 24, Loss G: -1.1675,Loss D: -0.2641\n",
      "Epoch 25, Loss G: -0.6752,Loss D: -0.1700\n",
      "Epoch 26, Loss G: -1.1881,Loss D:  0.0643\n",
      "Epoch 27, Loss G: -0.7482,Loss D: -0.1080\n",
      "Epoch 28, Loss G: -0.9440,Loss D: -0.0107\n",
      "Epoch 29, Loss G: -0.7580,Loss D: -0.1536\n",
      "Epoch 30, Loss G: -0.9671,Loss D: -0.0439\n",
      "Epoch 31, Loss G: -0.5064,Loss D: -0.4074\n",
      "Epoch 32, Loss G: -0.4858,Loss D: -0.1329\n",
      "Epoch 33, Loss G: -0.7305,Loss D: -0.4691\n",
      "Epoch 34, Loss G: -0.6143,Loss D: -0.2731\n",
      "Epoch 35, Loss G: -0.2699,Loss D: -0.2159\n",
      "Epoch 36, Loss G: -0.5824,Loss D: -0.2886\n",
      "Epoch 37, Loss G: -0.7441,Loss D: -0.3648\n",
      "Epoch 38, Loss G: -0.3543,Loss D: -0.0151\n",
      "Epoch 39, Loss G: -0.7844,Loss D: -0.0788\n",
      "Epoch 40, Loss G: -0.4944,Loss D: -0.2120\n",
      "Epoch 41, Loss G: -0.4974,Loss D: -0.0903\n",
      "Epoch 42, Loss G: -0.4967,Loss D: -0.1543\n",
      "Epoch 43, Loss G: -0.4646,Loss D:  0.0977\n",
      "Epoch 44, Loss G: -0.4798,Loss D: -0.1144\n",
      "Epoch 45, Loss G: -0.5222,Loss D: -0.1402\n",
      "Epoch 46, Loss G: -0.7394,Loss D:  0.0757\n",
      "Epoch 47, Loss G: -0.6333,Loss D: -0.1643\n",
      "Epoch 48, Loss G: -0.6109,Loss D:  0.0722\n",
      "Epoch 49, Loss G: -0.5874,Loss D: -0.1074\n",
      "Epoch 50, Loss G: -0.5085,Loss D: -0.1138\n",
      "Epoch 51, Loss G: -0.4210,Loss D: -0.3038\n",
      "Epoch 52, Loss G: -0.5141,Loss D: -0.0255\n",
      "Epoch 53, Loss G: -0.7830,Loss D: -0.4105\n",
      "Epoch 54, Loss G: -0.4411,Loss D: -0.0085\n",
      "Epoch 55, Loss G: -0.8316,Loss D: -0.2894\n",
      "Epoch 56, Loss G: -0.5945,Loss D: -0.3384\n",
      "Epoch 57, Loss G: -0.1575,Loss D: -0.0361\n",
      "Epoch 58, Loss G: -0.3761,Loss D: -0.0829\n",
      "Epoch 59, Loss G: -0.4139,Loss D: -0.3196\n",
      "Epoch 60, Loss G: -0.4073,Loss D: -0.3283\n",
      "Epoch 61, Loss G: -0.3903,Loss D:  0.0807\n",
      "Epoch 62, Loss G: -0.2961,Loss D:  0.0519\n",
      "Epoch 63, Loss G: -0.4963,Loss D: -0.4257\n",
      "Epoch 64, Loss G: -0.5287,Loss D: -0.1808\n",
      "Epoch 65, Loss G: -0.5435,Loss D: -0.2333\n",
      "Epoch 66, Loss G: -0.5222,Loss D: -0.1647\n",
      "Epoch 67, Loss G: -0.4860,Loss D: -0.4415\n",
      "Epoch 68, Loss G: -0.2013,Loss D: -0.1823\n",
      "Epoch 69, Loss G: -0.2437,Loss D: -0.0821\n",
      "Epoch 70, Loss G: -0.0931,Loss D: -0.1551\n",
      "Epoch 71, Loss G: -0.0673,Loss D:  0.2061\n",
      "Epoch 72, Loss G: -0.5369,Loss D: -0.4556\n",
      "Epoch 73, Loss G: -0.3625,Loss D:  0.0792\n",
      "Epoch 74, Loss G: -0.7008,Loss D: -0.0726\n",
      "Epoch 75, Loss G: -0.1825,Loss D:  0.1620\n",
      "Epoch 76, Loss G: -0.6459,Loss D: -0.2282\n",
      "Epoch 77, Loss G: -0.6718,Loss D: -0.4402\n",
      "Epoch 78, Loss G: -0.6153,Loss D: -0.3048\n",
      "Epoch 79, Loss G: -0.2837,Loss D:  0.0332\n",
      "Epoch 80, Loss G: -0.0415,Loss D: -0.0352\n",
      "Epoch 81, Loss G: -0.1253,Loss D:  0.1022\n",
      "Epoch 82, Loss G: -0.5164,Loss D: -0.0626\n",
      "Epoch 83, Loss G: -0.3784,Loss D: -0.3736\n",
      "Epoch 84, Loss G: -0.2002,Loss D: -0.2186\n",
      "Epoch 85, Loss G: -0.2860,Loss D:  0.0617\n",
      "Epoch 86, Loss G: -0.3441,Loss D: -0.2208\n",
      "Epoch 87, Loss G: -0.6744,Loss D: -0.4157\n",
      "Epoch 88, Loss G: -0.3651,Loss D: -0.0375\n",
      "Epoch 89, Loss G: -0.1126,Loss D:  0.1236\n",
      "Epoch 90, Loss G:  0.0756,Loss D: -0.1663\n",
      "Epoch 91, Loss G: -0.0876,Loss D:  0.0489\n",
      "Epoch 92, Loss G: -0.2868,Loss D:  0.3419\n",
      "Epoch 93, Loss G: -0.5514,Loss D: -0.2793\n",
      "Epoch 94, Loss G: -0.6070,Loss D: -0.3704\n",
      "Epoch 95, Loss G: -0.4164,Loss D: -0.4379\n",
      "Epoch 96, Loss G: -0.2256,Loss D: -0.3582\n",
      "Epoch 97, Loss G: -0.2871,Loss D: -0.1983\n",
      "Epoch 98, Loss G: -0.4738,Loss D: -0.2188\n",
      "Epoch 99, Loss G: -0.4107,Loss D: -0.2476\n",
      "Epoch 100, Loss G: -0.4345,Loss D: -0.6736\n",
      "Epoch 101, Loss G: -0.4384,Loss D: -0.0148\n",
      "Epoch 102, Loss G: -0.5066,Loss D: -0.1070\n",
      "Epoch 103, Loss G: -0.3243,Loss D: -0.3461\n",
      "Epoch 104, Loss G: -0.3192,Loss D: -0.1477\n",
      "Epoch 105, Loss G: -0.4528,Loss D: -0.6196\n",
      "Epoch 106, Loss G: -0.1920,Loss D: -0.1304\n",
      "Epoch 107, Loss G: -0.3382,Loss D:  0.0137\n",
      "Epoch 108, Loss G: -0.6447,Loss D: -0.1219\n",
      "Epoch 109, Loss G: -0.5780,Loss D:  0.1183\n",
      "Epoch 110, Loss G: -0.4620,Loss D: -0.0619\n",
      "Epoch 111, Loss G: -0.9813,Loss D: -0.1638\n",
      "Epoch 112, Loss G: -0.5989,Loss D: -0.1004\n",
      "Epoch 113, Loss G: -0.5231,Loss D: -0.4076\n",
      "Epoch 114, Loss G: -0.0011,Loss D: -0.0548\n",
      "Epoch 115, Loss G: -0.4226,Loss D: -0.0291\n",
      "Epoch 116, Loss G: -0.4883,Loss D: -0.3530\n",
      "Epoch 117, Loss G: -0.0875,Loss D: -0.3607\n",
      "Epoch 118, Loss G: -0.2122,Loss D: -0.3071\n",
      "Epoch 119, Loss G: -0.1953,Loss D: -0.1712\n",
      "Epoch 120, Loss G: -0.1481,Loss D: -0.2434\n",
      "Epoch 121, Loss G:  0.0350,Loss D: -0.4387\n",
      "Epoch 122, Loss G: -0.0533,Loss D: -0.1624\n",
      "Epoch 123, Loss G: -0.3879,Loss D: -0.3227\n",
      "Epoch 124, Loss G: -0.3267,Loss D:  0.0464\n",
      "Epoch 125, Loss G: -0.5266,Loss D: -0.2155\n",
      "Epoch 126, Loss G: -0.5800,Loss D: -0.0969\n",
      "Epoch 127, Loss G: -0.6546,Loss D: -0.2593\n",
      "Epoch 128, Loss G: -0.6395,Loss D: -0.2014\n",
      "Epoch 129, Loss G: -0.8032,Loss D: -0.2368\n",
      "Epoch 130, Loss G: -0.8324,Loss D:  0.0770\n",
      "Epoch 131, Loss G: -0.8096,Loss D: -0.2423\n",
      "Epoch 132, Loss G: -0.8819,Loss D:  0.0321\n",
      "Epoch 133, Loss G: -0.4402,Loss D: -0.2283\n",
      "Epoch 134, Loss G: -0.3029,Loss D: -0.2670\n",
      "Epoch 135, Loss G: -0.7010,Loss D: -0.1472\n",
      "Epoch 136, Loss G: -0.4620,Loss D: -0.0176\n",
      "Epoch 137, Loss G: -0.6707,Loss D:  0.2394\n",
      "Epoch 138, Loss G: -0.5521,Loss D: -0.1195\n",
      "Epoch 139, Loss G: -1.0602,Loss D: -0.2104\n",
      "Epoch 140, Loss G: -0.9683,Loss D: -0.0709\n",
      "Epoch 141, Loss G: -0.8965,Loss D: -0.4151\n",
      "Epoch 142, Loss G: -0.6977,Loss D: -0.1498\n",
      "Epoch 143, Loss G: -0.6823,Loss D: -0.2123\n",
      "Epoch 144, Loss G: -0.4761,Loss D:  0.1153\n",
      "Epoch 145, Loss G: -0.5204,Loss D: -0.0268\n",
      "Epoch 146, Loss G: -0.8108,Loss D: -0.2421\n",
      "Epoch 147, Loss G: -0.4167,Loss D: -0.1945\n",
      "Epoch 148, Loss G: -0.3549,Loss D: -0.1437\n",
      "Epoch 149, Loss G: -0.5896,Loss D:  0.0333\n",
      "Epoch 150, Loss G: -0.8065,Loss D:  0.0477\n",
      "Epoch 151, Loss G: -0.5507,Loss D: -0.0260\n",
      "Epoch 152, Loss G: -0.8264,Loss D: -0.2259\n",
      "Epoch 153, Loss G: -0.9091,Loss D: -0.0910\n",
      "Epoch 154, Loss G: -0.9418,Loss D:  0.1654\n",
      "Epoch 155, Loss G: -0.6373,Loss D: -0.0758\n",
      "Epoch 156, Loss G: -0.6285,Loss D: -0.1953\n",
      "Epoch 157, Loss G: -0.2866,Loss D:  0.0712\n",
      "Epoch 158, Loss G: -0.1081,Loss D: -0.3679\n",
      "Epoch 159, Loss G: -0.1008,Loss D:  0.0671\n",
      "Epoch 160, Loss G: -0.3946,Loss D: -0.1083\n",
      "Epoch 161, Loss G: -0.3111,Loss D: -0.1813\n",
      "Epoch 162, Loss G: -0.2932,Loss D: -0.0118\n",
      "Epoch 163, Loss G: -0.5795,Loss D: -0.1887\n",
      "Epoch 164, Loss G: -0.5808,Loss D:  0.0012\n",
      "Epoch 165, Loss G: -0.5173,Loss D: -0.3189\n",
      "Epoch 166, Loss G:  0.0968,Loss D: -0.1070\n",
      "Epoch 167, Loss G: -0.1787,Loss D: -0.4388\n",
      "Epoch 168, Loss G: -0.3372,Loss D: -0.1056\n",
      "Epoch 169, Loss G: -0.6230,Loss D:  0.1067\n",
      "Epoch 170, Loss G: -0.3993,Loss D: -0.1651\n",
      "Epoch 171, Loss G: -0.5094,Loss D: -0.4315\n",
      "Epoch 172, Loss G: -0.4636,Loss D:  0.2632\n",
      "Epoch 173, Loss G: -0.4907,Loss D: -0.2545\n",
      "Epoch 174, Loss G: -0.3549,Loss D: -0.0294\n",
      "Epoch 175, Loss G: -0.1653,Loss D: -0.0016\n",
      "Epoch 176, Loss G: -0.5348,Loss D:  0.4383\n",
      "Epoch 177, Loss G: -0.1358,Loss D:  0.1231\n",
      "Epoch 178, Loss G: -0.5487,Loss D: -0.3614\n",
      "Epoch 179, Loss G: -0.1175,Loss D: -0.2036\n",
      "Epoch 180, Loss G: -0.0131,Loss D: -0.2538\n",
      "Epoch 181, Loss G: -0.6898,Loss D:  0.1743\n",
      "Epoch 182, Loss G: -0.7075,Loss D:  0.0358\n",
      "Epoch 183, Loss G: -0.2275,Loss D: -0.1115\n",
      "Epoch 184, Loss G: -0.2534,Loss D: -0.2970\n",
      "Epoch 185, Loss G: -0.3423,Loss D: -0.2823\n",
      "Epoch 186, Loss G: -0.8519,Loss D: -0.0091\n",
      "Epoch 187, Loss G: -0.8118,Loss D: -0.5135\n",
      "Epoch 188, Loss G: -0.4834,Loss D: -0.3445\n",
      "Epoch 189, Loss G: -0.7609,Loss D: -0.3969\n",
      "Epoch 190, Loss G: -0.7910,Loss D:  0.0251\n",
      "Epoch 191, Loss G: -0.4421,Loss D:  0.0827\n",
      "Epoch 192, Loss G: -0.5781,Loss D: -0.5282\n",
      "Epoch 193, Loss G: -0.3351,Loss D: -0.2709\n",
      "Epoch 194, Loss G: -0.3656,Loss D: -0.2156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, Loss G: -0.5261,Loss D: -0.0075\n",
      "Epoch 196, Loss G: -0.5167,Loss D: -0.0146\n",
      "Epoch 197, Loss G: -0.3848,Loss D:  0.0450\n",
      "Epoch 198, Loss G: -0.4742,Loss D: -0.0643\n",
      "Epoch 199, Loss G: -0.2714,Loss D: -0.3152\n",
      "Epoch 200, Loss G: -0.6815,Loss D:  0.0548\n",
      "Epoch 201, Loss G: -0.3424,Loss D: -0.7198\n",
      "Epoch 202, Loss G: -0.4733,Loss D: -0.2780\n",
      "Epoch 203, Loss G: -0.4560,Loss D: -0.1680\n",
      "Epoch 204, Loss G: -0.4523,Loss D: -0.2623\n",
      "Epoch 205, Loss G: -0.3346,Loss D: -0.2298\n",
      "Epoch 206, Loss G: -0.3219,Loss D: -0.0520\n",
      "Epoch 207, Loss G: -0.6410,Loss D:  0.0222\n",
      "Epoch 208, Loss G: -0.6945,Loss D: -0.3104\n",
      "Epoch 209, Loss G: -0.3281,Loss D: -0.1552\n",
      "Epoch 210, Loss G: -0.3758,Loss D:  0.0352\n",
      "Epoch 211, Loss G: -0.5997,Loss D: -0.2539\n",
      "Epoch 212, Loss G: -0.9433,Loss D: -0.2055\n",
      "Epoch 213, Loss G: -0.3669,Loss D: -0.2796\n",
      "Epoch 214, Loss G: -0.6740,Loss D: -0.5681\n",
      "Epoch 215, Loss G: -0.3460,Loss D: -0.1960\n",
      "Epoch 216, Loss G: -0.4993,Loss D:  0.0812\n",
      "Epoch 217, Loss G: -0.2390,Loss D:  0.0715\n",
      "Epoch 218, Loss G: -0.2190,Loss D:  0.0581\n",
      "Epoch 219, Loss G: -0.4999,Loss D: -0.1057\n",
      "Epoch 220, Loss G: -0.4327,Loss D: -0.2667\n",
      "Epoch 221, Loss G: -0.1117,Loss D: -0.4024\n",
      "Epoch 222, Loss G: -0.2729,Loss D: -0.2394\n",
      "Epoch 223, Loss G: -0.9382,Loss D: -0.2359\n",
      "Epoch 224, Loss G: -0.6576,Loss D: -0.1543\n",
      "Epoch 225, Loss G: -0.6933,Loss D:  0.1362\n",
      "Epoch 226, Loss G: -0.6789,Loss D:  0.0653\n",
      "Epoch 227, Loss G: -0.9076,Loss D: -0.1465\n",
      "Epoch 228, Loss G: -0.7067,Loss D: -0.0455\n",
      "Epoch 229, Loss G: -0.6092,Loss D: -0.2768\n",
      "Epoch 230, Loss G: -0.4421,Loss D: -0.3197\n",
      "Epoch 231, Loss G: -0.4451,Loss D: -0.0494\n",
      "Epoch 232, Loss G: -0.4267,Loss D: -0.3240\n",
      "Epoch 233, Loss G: -0.6708,Loss D: -0.2173\n",
      "Epoch 234, Loss G: -0.7320,Loss D:  0.1204\n",
      "Epoch 235, Loss G: -0.3844,Loss D: -0.2552\n",
      "Epoch 236, Loss G: -0.1323,Loss D:  0.0908\n",
      "Epoch 237, Loss G: -0.5989,Loss D: -0.2574\n",
      "Epoch 238, Loss G: -0.7756,Loss D: -0.1392\n",
      "Epoch 239, Loss G: -0.5036,Loss D:  0.0522\n",
      "Epoch 240, Loss G: -1.2277,Loss D: -0.0970\n",
      "Epoch 241, Loss G: -0.4960,Loss D: -0.0785\n",
      "Epoch 242, Loss G: -0.4453,Loss D: -0.3259\n",
      "Epoch 243, Loss G: -0.6760,Loss D:  0.0365\n",
      "Epoch 244, Loss G: -0.7019,Loss D: -0.0792\n",
      "Epoch 245, Loss G: -0.7156,Loss D: -0.0287\n",
      "Epoch 246, Loss G: -0.7414,Loss D:  0.1374\n",
      "Epoch 247, Loss G: -0.6613,Loss D: -0.2397\n",
      "Epoch 248, Loss G: -0.4174,Loss D: -0.0842\n",
      "Epoch 249, Loss G: -0.6416,Loss D:  0.2292\n",
      "Epoch 250, Loss G: -0.6455,Loss D: -0.0473\n",
      "Epoch 251, Loss G: -0.8171,Loss D: -0.3052\n",
      "Epoch 252, Loss G: -1.1034,Loss D: -0.1164\n",
      "Epoch 253, Loss G: -0.5303,Loss D: -0.0332\n",
      "Epoch 254, Loss G: -0.8259,Loss D: -0.0488\n",
      "Epoch 255, Loss G: -0.8273,Loss D: -0.1455\n",
      "Epoch 256, Loss G: -1.0085,Loss D: -0.1423\n",
      "Epoch 257, Loss G: -0.9939,Loss D:  0.1704\n",
      "Epoch 258, Loss G: -0.8204,Loss D: -0.1964\n",
      "Epoch 259, Loss G: -0.8056,Loss D:  0.1393\n",
      "Epoch 260, Loss G: -0.5755,Loss D: -0.2698\n",
      "Epoch 261, Loss G: -0.5882,Loss D: -0.0160\n",
      "Epoch 262, Loss G: -0.7021,Loss D: -0.0676\n",
      "Epoch 263, Loss G: -1.1396,Loss D: -0.4111\n",
      "Epoch 264, Loss G: -0.6160,Loss D:  0.0253\n",
      "Epoch 265, Loss G: -0.5163,Loss D: -0.3519\n",
      "Epoch 266, Loss G: -0.9575,Loss D: -0.1245\n",
      "Epoch 267, Loss G: -0.6242,Loss D: -0.1941\n",
      "Epoch 268, Loss G: -0.7371,Loss D: -0.2783\n",
      "Epoch 269, Loss G: -0.4675,Loss D: -0.0639\n",
      "Epoch 270, Loss G: -0.6273,Loss D:  0.0723\n",
      "Epoch 271, Loss G: -0.4682,Loss D: -0.1475\n",
      "Epoch 272, Loss G: -0.5386,Loss D: -0.1193\n",
      "Epoch 273, Loss G: -0.8612,Loss D: -0.0678\n",
      "Epoch 274, Loss G: -0.6992,Loss D: -0.2288\n",
      "Epoch 275, Loss G: -0.8774,Loss D: -0.3205\n",
      "Epoch 276, Loss G: -0.7236,Loss D:  0.2234\n",
      "Epoch 277, Loss G: -0.7263,Loss D: -0.0645\n",
      "Epoch 278, Loss G: -0.4903,Loss D: -0.1171\n",
      "Epoch 279, Loss G: -0.7958,Loss D: -0.0293\n",
      "Epoch 280, Loss G: -0.6850,Loss D: -0.2408\n",
      "Epoch 281, Loss G: -0.3454,Loss D: -0.3174\n",
      "Epoch 282, Loss G: -0.6004,Loss D: -0.0192\n",
      "Epoch 283, Loss G: -0.6993,Loss D: -0.2943\n",
      "Epoch 284, Loss G: -0.6387,Loss D: -0.0375\n",
      "Epoch 285, Loss G: -0.7774,Loss D:  0.0125\n",
      "Epoch 286, Loss G: -0.4734,Loss D: -0.0649\n",
      "Epoch 287, Loss G: -0.7862,Loss D: -0.0383\n",
      "Epoch 288, Loss G: -0.8359,Loss D: -0.0794\n",
      "Epoch 289, Loss G: -0.7114,Loss D: -0.1252\n",
      "Epoch 290, Loss G: -0.5785,Loss D: -0.2530\n",
      "Epoch 291, Loss G: -0.7966,Loss D: -0.2145\n",
      "Epoch 292, Loss G: -0.4502,Loss D: -0.2386\n",
      "Epoch 293, Loss G: -0.6392,Loss D: -0.0711\n",
      "Epoch 294, Loss G: -0.8231,Loss D: -0.0644\n",
      "Epoch 295, Loss G: -0.7606,Loss D: -0.5262\n",
      "Epoch 296, Loss G: -0.7451,Loss D: -0.0792\n",
      "Epoch 297, Loss G: -0.9466,Loss D:  0.1131\n",
      "Epoch 298, Loss G: -0.8512,Loss D:  0.2109\n",
      "Epoch 299, Loss G: -1.2204,Loss D: -0.1842\n",
      "Epoch 300, Loss G: -0.9649,Loss D:  0.1763\n",
      "Epoch 301, Loss G: -0.5643,Loss D: -0.2560\n",
      "Epoch 302, Loss G: -0.6294,Loss D:  0.1711\n",
      "Epoch 303, Loss G: -0.5973,Loss D: -0.1083\n",
      "Epoch 304, Loss G: -0.8768,Loss D: -0.1616\n",
      "Epoch 305, Loss G: -0.6200,Loss D: -0.2762\n",
      "Epoch 306, Loss G: -0.8998,Loss D:  0.0356\n",
      "Epoch 307, Loss G: -0.7164,Loss D: -0.0432\n",
      "Epoch 308, Loss G: -0.8320,Loss D: -0.1855\n",
      "Epoch 309, Loss G: -0.6331,Loss D: -0.1842\n",
      "Epoch 310, Loss G: -0.5868,Loss D: -0.0810\n",
      "Epoch 311, Loss G: -0.5636,Loss D: -0.1205\n",
      "Epoch 312, Loss G: -0.6761,Loss D: -0.2816\n",
      "Epoch 313, Loss G: -0.3690,Loss D: -0.2160\n",
      "Epoch 314, Loss G: -0.6106,Loss D: -0.1428\n",
      "Epoch 315, Loss G: -0.5485,Loss D: -0.0411\n",
      "Epoch 316, Loss G: -0.6027,Loss D: -0.0350\n",
      "Epoch 317, Loss G: -0.4996,Loss D: -0.1543\n",
      "Epoch 318, Loss G: -0.1570,Loss D: -0.1759\n",
      "Epoch 319, Loss G: -0.6600,Loss D:  0.1328\n",
      "Epoch 320, Loss G: -0.9173,Loss D:  0.1161\n",
      "Epoch 321, Loss G: -0.4722,Loss D: -0.0749\n",
      "Epoch 322, Loss G: -0.4981,Loss D: -0.2932\n",
      "Epoch 323, Loss G: -0.4490,Loss D:  0.0057\n",
      "Epoch 324, Loss G: -0.6763,Loss D: -0.1857\n",
      "Epoch 325, Loss G: -0.8839,Loss D: -0.2102\n",
      "Epoch 326, Loss G: -0.6263,Loss D:  0.0198\n",
      "Epoch 327, Loss G: -0.5369,Loss D:  0.0153\n",
      "Epoch 328, Loss G: -0.5256,Loss D:  0.0266\n",
      "Epoch 329, Loss G: -0.5156,Loss D: -0.1352\n",
      "Epoch 330, Loss G: -0.5898,Loss D: -0.1710\n",
      "Epoch 331, Loss G: -0.5245,Loss D: -0.4147\n",
      "Epoch 332, Loss G: -0.4557,Loss D: -0.0399\n",
      "Epoch 333, Loss G: -0.5902,Loss D: -0.0334\n",
      "Epoch 334, Loss G: -0.7583,Loss D: -0.3484\n",
      "Epoch 335, Loss G: -0.8729,Loss D: -0.0140\n",
      "Epoch 336, Loss G: -0.8809,Loss D: -0.1483\n",
      "Epoch 337, Loss G: -0.6062,Loss D: -0.0138\n",
      "Epoch 338, Loss G: -0.2344,Loss D: -0.0850\n",
      "Epoch 339, Loss G: -0.5057,Loss D:  0.0247\n",
      "Epoch 340, Loss G: -0.4425,Loss D: -0.0968\n",
      "Epoch 341, Loss G: -0.3537,Loss D: -0.2031\n",
      "Epoch 342, Loss G: -0.7123,Loss D: -0.2024\n",
      "Epoch 343, Loss G: -0.3680,Loss D: -0.1018\n",
      "Epoch 344, Loss G: -0.7860,Loss D: -0.0502\n",
      "Epoch 345, Loss G: -0.5115,Loss D: -0.1396\n",
      "Epoch 346, Loss G: -0.8126,Loss D:  0.0471\n",
      "Epoch 347, Loss G: -0.7758,Loss D:  0.0746\n",
      "Epoch 348, Loss G: -0.8493,Loss D: -0.2248\n",
      "Epoch 349, Loss G: -0.8669,Loss D:  0.0635\n",
      "Epoch 350, Loss G: -0.8235,Loss D: -0.3770\n",
      "Epoch 351, Loss G: -0.7427,Loss D: -0.2254\n",
      "Epoch 352, Loss G: -0.5808,Loss D: -0.1055\n",
      "Epoch 353, Loss G: -0.8540,Loss D:  0.1020\n",
      "Epoch 354, Loss G: -0.7492,Loss D: -0.0616\n",
      "Epoch 355, Loss G: -0.9062,Loss D: -0.2115\n",
      "Epoch 356, Loss G: -0.8342,Loss D: -0.1366\n",
      "Epoch 357, Loss G: -0.6910,Loss D:  0.0025\n",
      "Epoch 358, Loss G: -0.6918,Loss D: -0.1613\n",
      "Epoch 359, Loss G: -0.5704,Loss D:  0.0319\n",
      "Epoch 360, Loss G: -0.7480,Loss D: -0.2128\n",
      "Epoch 361, Loss G: -0.6113,Loss D: -0.1072\n",
      "Epoch 362, Loss G: -0.4710,Loss D:  0.1014\n",
      "Epoch 363, Loss G: -0.5148,Loss D: -0.2196\n",
      "Epoch 364, Loss G: -0.5663,Loss D:  0.0567\n",
      "Epoch 365, Loss G: -0.4160,Loss D: -0.2630\n",
      "Epoch 366, Loss G: -0.6312,Loss D: -0.4768\n",
      "Epoch 367, Loss G: -0.9290,Loss D: -0.2357\n",
      "Epoch 368, Loss G: -0.8862,Loss D: -0.2141\n",
      "Epoch 369, Loss G: -0.7190,Loss D: -0.0571\n",
      "Epoch 370, Loss G: -0.5764,Loss D: -0.0525\n",
      "Epoch 371, Loss G: -0.6775,Loss D: -0.2179\n",
      "Epoch 372, Loss G: -0.6564,Loss D: -0.1616\n",
      "Epoch 373, Loss G: -0.7669,Loss D: -0.0077\n",
      "Epoch 374, Loss G: -0.6540,Loss D: -0.2500\n",
      "Epoch 375, Loss G: -0.4725,Loss D: -0.5374\n",
      "Epoch 376, Loss G: -0.2874,Loss D:  0.0967\n",
      "Epoch 377, Loss G: -0.6336,Loss D: -0.0083\n",
      "Epoch 378, Loss G: -0.9344,Loss D: -0.1784\n",
      "Epoch 379, Loss G: -0.7272,Loss D: -0.2735\n",
      "Epoch 380, Loss G: -0.4143,Loss D: -0.2263\n",
      "Epoch 381, Loss G: -0.7541,Loss D: -0.0784\n",
      "Epoch 382, Loss G: -0.6474,Loss D:  0.0589\n",
      "Epoch 383, Loss G: -0.8396,Loss D:  0.0181\n",
      "Epoch 384, Loss G: -0.7923,Loss D: -0.2155\n",
      "Epoch 385, Loss G: -0.3806,Loss D:  0.0138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386, Loss G: -0.7364,Loss D: -0.0137\n",
      "Epoch 387, Loss G: -0.8928,Loss D:  0.2607\n",
      "Epoch 388, Loss G: -0.7541,Loss D: -0.1009\n",
      "Epoch 389, Loss G: -0.5556,Loss D: -0.0925\n",
      "Epoch 390, Loss G: -0.5772,Loss D: -0.2002\n",
      "Epoch 391, Loss G: -0.6142,Loss D:  0.1093\n",
      "Epoch 392, Loss G: -0.8911,Loss D:  0.0897\n",
      "Epoch 393, Loss G: -0.6778,Loss D: -0.0927\n",
      "Epoch 394, Loss G: -0.7712,Loss D: -0.0347\n",
      "Epoch 395, Loss G: -0.8085,Loss D: -0.0481\n",
      "Epoch 396, Loss G: -1.0440,Loss D: -0.2046\n",
      "Epoch 397, Loss G: -0.6481,Loss D: -0.1431\n",
      "Epoch 398, Loss G: -0.8562,Loss D: -0.0813\n",
      "Epoch 399, Loss G: -0.7476,Loss D:  0.0745\n",
      "Epoch 400, Loss G: -0.6510,Loss D: -0.1932\n",
      "Epoch 401, Loss G: -1.1282,Loss D: -0.0421\n",
      "Epoch 402, Loss G: -0.9055,Loss D: -0.2322\n",
      "Epoch 403, Loss G: -0.7069,Loss D: -0.3115\n",
      "Epoch 404, Loss G: -0.9516,Loss D:  0.1022\n",
      "Epoch 405, Loss G: -0.7850,Loss D:  0.0154\n",
      "Epoch 406, Loss G: -1.0781,Loss D:  0.1001\n",
      "Epoch 407, Loss G: -1.0614,Loss D: -0.0170\n",
      "Epoch 408, Loss G: -0.8128,Loss D: -0.1404\n",
      "Epoch 409, Loss G: -1.0384,Loss D: -0.0797\n",
      "Epoch 410, Loss G: -0.6959,Loss D: -0.1580\n",
      "Epoch 411, Loss G: -0.7516,Loss D: -0.2882\n",
      "Epoch 412, Loss G: -0.6375,Loss D: -0.2501\n",
      "Epoch 413, Loss G: -0.7480,Loss D: -0.0278\n",
      "Epoch 414, Loss G: -0.6893,Loss D: -0.3788\n",
      "Epoch 415, Loss G: -1.0022,Loss D: -0.1790\n",
      "Epoch 416, Loss G: -0.8015,Loss D: -0.2292\n",
      "Epoch 417, Loss G: -0.6518,Loss D: -0.2482\n",
      "Epoch 418, Loss G: -0.7536,Loss D: -0.1560\n",
      "Epoch 419, Loss G: -0.9372,Loss D:  0.0273\n",
      "Epoch 420, Loss G: -0.9219,Loss D:  0.1316\n",
      "Epoch 421, Loss G: -0.8804,Loss D:  0.0381\n",
      "Epoch 422, Loss G: -0.6959,Loss D:  0.0212\n",
      "Epoch 423, Loss G: -0.8587,Loss D:  0.0528\n",
      "Epoch 424, Loss G: -0.9473,Loss D: -0.0717\n",
      "Epoch 425, Loss G: -0.8413,Loss D: -0.1393\n",
      "Epoch 426, Loss G: -0.9416,Loss D: -0.0018\n",
      "Epoch 427, Loss G: -0.7469,Loss D: -0.2611\n",
      "Epoch 428, Loss G: -0.7805,Loss D: -0.3774\n",
      "Epoch 429, Loss G: -0.8574,Loss D:  0.1168\n",
      "Epoch 430, Loss G: -1.0202,Loss D: -0.1102\n",
      "Epoch 431, Loss G: -0.8864,Loss D: -0.0997\n",
      "Epoch 432, Loss G: -0.9451,Loss D: -0.0897\n",
      "Epoch 433, Loss G: -0.3758,Loss D: -0.1606\n",
      "Epoch 434, Loss G: -0.8660,Loss D:  0.1298\n",
      "Epoch 435, Loss G: -0.8638,Loss D: -0.0851\n",
      "Epoch 436, Loss G: -1.1409,Loss D: -0.0190\n",
      "Epoch 437, Loss G: -1.1163,Loss D: -0.1180\n",
      "Epoch 438, Loss G: -0.9152,Loss D: -0.0084\n",
      "Epoch 439, Loss G: -0.9934,Loss D: -0.1863\n",
      "Epoch 440, Loss G: -0.8895,Loss D: -0.0091\n",
      "Epoch 441, Loss G: -1.0073,Loss D: -0.0707\n",
      "Epoch 442, Loss G: -1.0297,Loss D: -0.1256\n",
      "Epoch 443, Loss G: -0.9719,Loss D: -0.0195\n",
      "Epoch 444, Loss G: -0.9477,Loss D: -0.2761\n",
      "Epoch 445, Loss G: -1.1287,Loss D:  0.0632\n",
      "Epoch 446, Loss G: -0.7159,Loss D: -0.1454\n",
      "Epoch 447, Loss G: -0.9034,Loss D:  0.0002\n",
      "Epoch 448, Loss G: -0.7703,Loss D: -0.1824\n",
      "Epoch 449, Loss G: -0.9769,Loss D: -0.1771\n",
      "Epoch 450, Loss G: -0.6605,Loss D: -0.1582\n",
      "Epoch 451, Loss G: -0.6428,Loss D: -0.0729\n",
      "Epoch 452, Loss G: -0.8198,Loss D: -0.1791\n",
      "Epoch 453, Loss G: -0.9181,Loss D:  0.0289\n",
      "Epoch 454, Loss G: -0.9880,Loss D: -0.1756\n",
      "Epoch 455, Loss G: -0.6417,Loss D: -0.1599\n",
      "Epoch 456, Loss G: -0.7837,Loss D: -0.2468\n",
      "Epoch 457, Loss G: -0.7345,Loss D: -0.2645\n",
      "Epoch 458, Loss G: -0.8633,Loss D:  0.0126\n",
      "Epoch 459, Loss G: -1.0630,Loss D: -0.0634\n",
      "Epoch 460, Loss G: -0.8384,Loss D: -0.0728\n",
      "Epoch 461, Loss G: -0.8299,Loss D: -0.2991\n",
      "Epoch 462, Loss G: -0.7885,Loss D: -0.0955\n",
      "Epoch 463, Loss G: -0.9614,Loss D:  0.1101\n",
      "Epoch 464, Loss G: -0.8668,Loss D: -0.1121\n",
      "Epoch 465, Loss G: -0.6227,Loss D: -0.0367\n",
      "Epoch 466, Loss G: -0.6467,Loss D: -0.1003\n",
      "Epoch 467, Loss G: -0.7523,Loss D: -0.0288\n",
      "Epoch 468, Loss G: -0.7975,Loss D: -0.2323\n",
      "Epoch 469, Loss G: -0.8361,Loss D: -0.1475\n",
      "Epoch 470, Loss G: -0.8292,Loss D: -0.2002\n",
      "Epoch 471, Loss G: -0.7344,Loss D: -0.0477\n",
      "Epoch 472, Loss G: -0.7198,Loss D:  0.0321\n",
      "Epoch 473, Loss G: -0.8787,Loss D: -0.0043\n",
      "Epoch 474, Loss G: -0.5508,Loss D: -0.2923\n",
      "Epoch 475, Loss G: -0.6949,Loss D: -0.2942\n",
      "Epoch 476, Loss G: -0.5445,Loss D:  0.0189\n",
      "Epoch 477, Loss G: -0.4933,Loss D: -0.1506\n",
      "Epoch 478, Loss G: -0.5221,Loss D: -0.2732\n",
      "Epoch 479, Loss G: -0.8407,Loss D: -0.0296\n",
      "Epoch 480, Loss G: -0.3302,Loss D: -0.0234\n",
      "Epoch 481, Loss G: -0.6012,Loss D: -0.0608\n",
      "Epoch 482, Loss G: -0.6607,Loss D: -0.0055\n",
      "Epoch 483, Loss G: -0.9576,Loss D:  0.1378\n",
      "Epoch 484, Loss G: -0.3988,Loss D: -0.0381\n",
      "Epoch 485, Loss G: -0.5267,Loss D:  0.1134\n",
      "Epoch 486, Loss G: -0.5585,Loss D:  0.1148\n",
      "Epoch 487, Loss G: -0.5147,Loss D: -0.0771\n",
      "Epoch 488, Loss G: -0.7761,Loss D: -0.2823\n",
      "Epoch 489, Loss G: -0.6560,Loss D:  0.0371\n",
      "Epoch 490, Loss G: -0.7073,Loss D: -0.1298\n",
      "Epoch 491, Loss G: -0.5977,Loss D:  0.0604\n",
      "Epoch 492, Loss G: -0.6338,Loss D: -0.1378\n",
      "Epoch 493, Loss G: -0.6375,Loss D:  0.1188\n",
      "Epoch 494, Loss G: -0.9230,Loss D:  0.0789\n",
      "Epoch 495, Loss G: -0.3480,Loss D: -0.1147\n",
      "Epoch 496, Loss G: -0.3569,Loss D: -0.2521\n",
      "Epoch 497, Loss G: -0.3042,Loss D: -0.1597\n",
      "Epoch 498, Loss G: -0.5156,Loss D: -0.0595\n",
      "Epoch 499, Loss G: -0.5293,Loss D:  0.2267\n",
      "Epoch 500, Loss G: -0.8768,Loss D:  0.1429\n"
     ]
    }
   ],
   "source": [
    "# train CTGAN and generate fake data\n",
    "ctgan = CTGANSynthesizer(verbose=True)\n",
    "ctgan.fit(GAN_data_train, adult_discrete_columns, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    39123\n",
       "1    10877\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create fake samples\n",
    "samples = ctgan.sample(50000)\n",
    "samples['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>education_10th</th>\n",
       "      <th>education_11th</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass_Federal-gov  workclass_Local-gov  workclass_Never-worked  \\\n",
       "0                      0                    1                       0   \n",
       "1                      0                    0                       0   \n",
       "2                      0                    0                       0   \n",
       "3                      0                    0                       0   \n",
       "4                      0                    0                       0   \n",
       "\n",
       "   workclass_Private  workclass_Self-emp-inc  workclass_Self-emp-not-inc  \\\n",
       "0                  0                       0                           0   \n",
       "1                  1                       0                           0   \n",
       "2                  1                       0                           0   \n",
       "3                  0                       0                           1   \n",
       "4                  1                       0                           0   \n",
       "\n",
       "   workclass_State-gov  workclass_Without-pay  education_10th  education_11th  \\\n",
       "0                    0                      0               0               0   \n",
       "1                    0                      0               0               0   \n",
       "2                    0                      0               0               0   \n",
       "3                    0                      0               0               0   \n",
       "4                    0                      0               0               0   \n",
       "\n",
       "   ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0  ...                        0                           0   \n",
       "1  ...                        0                           0   \n",
       "2  ...                        0                           0   \n",
       "3  ...                        0                           0   \n",
       "4  ...                        0                           0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        0                     0                      0   \n",
       "1                        0                     0                      0   \n",
       "2                        0                     0                      0   \n",
       "3                        0                     0                      0   \n",
       "4                        0                     0                      0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        0                               0   \n",
       "1                        0                               0   \n",
       "2                        0                               0   \n",
       "3                        0                               0   \n",
       "4                        0                               0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                             1                       0   \n",
       "1                             1                       0   \n",
       "2                             1                       0   \n",
       "3                             1                       0   \n",
       "4                             1                       0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the Ruiwen's Encoder for ML utility and CTGAN transformer for GAN\n",
    "str_cols= [ 'workclass', 'education', 'marital-status', 'relationship','race', 'sex','native-country']\n",
    "num_cols = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week', 'label']\n",
    "dataframe = pd.DataFrame(samples.loc[:,str_cols])\n",
    "\n",
    "one_hot_columns = pd.DataFrame()\n",
    "for col_name, item in dataframe.iteritems():\n",
    "    \n",
    "    #print(col_name)\n",
    "    #print(item)\n",
    "    col = pd.get_dummies(item, prefix=col_name)\n",
    "    one_hot_columns =pd.concat([one_hot_columns,col],axis=1)\n",
    "one_hot_columns.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire data is concat of discrete and contiuous cols\n",
    "fake_all = pd.concat([one_hot_columns,samples.loc[:,num_cols]],axis=1)\n",
    "#adult_data_all.head()\n",
    "fake_X = fake_all.drop([\"label\"],axis=1)\n",
    "fake_X, fake_y = fake_X,fake_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure cols of generated data has the same index sort \n",
    "fake_X = fake_X.reindex(adult_data_all.columns,axis=1, fill_value=0) # fill new cols with 0\n",
    "#len(fake_X.columns)\n",
    "fake_X = fake_X.drop([\"label\"],axis=1) # separate X and Y for training\n",
    "fake_X, fake_y = fake_X,fake_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the GAN generated train data:  (50000, 91)\n",
      "shape of the GAN generated labels:  (50000,)\n",
      "shape of the original test data:  (14653, 91)\n",
      "number of events in the fake data:  10877\n"
     ]
    }
   ],
   "source": [
    "# check the training data shape to agree with the original data (# of features/cols)\n",
    "print('shape of the GAN generated train data: ',fake_X.shape)\n",
    "print('shape of the GAN generated labels: ',fake_y.shape)\n",
    "print('shape of the original test data: ',orig_X_test.shape)\n",
    "print('number of events in the fake data: ', sum(fake_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the GAN generated data:\n",
      "Decision Tree Acc:  0.7976523578789326 f-1:  0.33295838020247476 AUC: 0.841645417607766\n",
      "Linear SVM Acc:  0.3564457790213608 f-1:  0.37133333333333335 AUC: 0.4867450664095051\n",
      "Random Forest Acc:  0.8259742032348325 f-1:  0.5513722730471499 AUC: 0.8808836855255372\n",
      "Logistic Regression Acc:  0.7958097317955367 f-1:  0.3415492957746479 AUC: 0.6166703419435491\n",
      "MLP Acc:  0.5426875042653382 f-1:  0.47073690861701284 AUC: 0.7231856447331998\n"
     ]
    }
   ],
   "source": [
    "# train a classifier on the CTGAN generated data\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]\n",
    "\n",
    "print('ML scores for the GAN generated data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(fake_X, fake_y)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
